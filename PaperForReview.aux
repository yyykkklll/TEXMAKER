\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{jiangCrossModalImplicitRelation2023,yanLearningComprehensiveRepresentations2023,shaoLearningGranularityUnifiedRepresentations2022,dingSemanticallySelfAlignedNetwork2021}
\citation{bukhariLanguageVisionBased2023}
\citation{galiyawalaPersonRetrievalSurveillance2021}
\citation{dosovitskiyImageWorth16x162021,tanHarnessingPowerMLLMs2024,zuoPLIPLanguageImagePretraining2024,yangUnifiedTextbasedPerson2023,heInstructReIDUniversalPurpose2025,liPromptDecouplingTexttoImage2024}
\citation{radfordLearningTransferableVisual2021,yaoFILIPFinegrainedInteractive2021,niuLLMLocBootstrapSingleimage2025,zhaoLuoJiaHOGHierarchyOriented2024}
\citation{jiangModelingThousandsHuman2025}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.~Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{\hskip -1em.~Introduction}{figure.caption.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{[1][1][]1}{}{}{}}
\@writefile{brf}{\backcite{jiangCrossModalImplicitRelation2023, yanLearningComprehensiveRepresentations2023,shaoLearningGranularityUnifiedRepresentations2022, dingSemanticallySelfAlignedNetwork2021}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{bukhariLanguageVisionBased2023}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{galiyawalaPersonRetrievalSurveillance2021}{{1}{1}{figure.caption.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The figure compares the structural differences between the traditional TPRe-ID method and the method proposed in this paper: (a) The traditional method encodes the image and text separately and then directly fuses them without distinguishing between identity and non-identity information, which limits the semantic alignment effect; (b) The method in this paper introduces a decoupling module, which divides the image features into identity and clothing, and aligns them with the corresponding descriptions generated by the multimodal large language model to achieve more fine-grained semantic matching and improve recognition capabilities in complex scenarios.}}{1}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:figure1}{{1}{1}{The figure compares the structural differences between the traditional TPRe-ID method and the method proposed in this paper: (a) The traditional method encodes the image and text separately and then directly fuses them without distinguishing between identity and non-identity information, which limits the semantic alignment effect; (b) The method in this paper introduces a decoupling module, which divides the image features into identity and clothing, and aligns them with the corresponding descriptions generated by the multimodal large language model to achieve more fine-grained semantic matching and improve recognition capabilities in complex scenarios}{figure.caption.1}{}}
\newlabel{fig:figure1@cref}{{[figure][1][]1}{[1][1][]1}{}{}{}}
\@writefile{brf}{\backcite{dosovitskiyImageWorth16x162021,tanHarnessingPowerMLLMs2024,zuoPLIPLanguageImagePretraining2024,yangUnifiedTextbasedPerson2023,heInstructReIDUniversalPurpose2025,liPromptDecouplingTexttoImage2024}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{radfordLearningTransferableVisual2021,yaoFILIPFinegrainedInteractive2021,niuLLMLocBootstrapSingleimage2025,zhaoLuoJiaHOGHierarchyOriented2024}{{1}{1}{figure.caption.1}}}
\citation{wangLearningDeepStructurePreserving2016,wangLanguagePersonSearch2019,liPersonSearchNatural2017}
\citation{gaoContextualNonLocalAlignment2021,wangCAIBCCapturingAllround2022}
\citation{wangViTAAVisualTextualAttributes2020,dingSemanticallySelfAlignedNetwork2021}
\citation{jiangCrossModalImplicitRelation2023,shaoLearningGranularityUnifiedRepresentations2022}
\citation{tranDisentangledRepresentationLearning2017}
\citation{yuan2021transformer}
\citation{charpentierStrongerReidentificationAttacks2025,liuMoAVRMixtureofAgentsSystem2025}
\citation{wangDisentangledRepresentationLearning2024}
\citation{liuMultitaskAdversarialNetwork2018}
\citation{sanchezLearningDisentangledRepresentations2019}
\citation{chengImprovingDisentangledText2020}
\citation{materzynskaDisentanglingVisualWritten2022}
\citation{liuMultitaskAdversarialNetwork2018}
\citation{chengDisentangledFeatureRepresentation2021}
\citation{ganinUnsupervisedDomainAdaptation2015}
\citation{materzynskaDisentanglingVisualWritten2022}
\citation{liDisentanglingIdentityFeatures2024,azadActivityBiometricsPersonIdentification2024}
\citation{baiDisentangledFeatureLearning2020}
\citation{Cui}
\citation{liDisentanglingIdentityFeatures2024}
\@writefile{brf}{\backcite{jiangModelingThousandsHuman2025}{{2}{1}{figure.caption.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.~Related Work}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\hskip -1em.~Text-to-Image Re-ID}{2}{subsection.2.1}\protected@file@percent }
\@writefile{brf}{\backcite{wangLearningDeepStructurePreserving2016,wangLanguagePersonSearch2019,liPersonSearchNatural2017}{{2}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{gaoContextualNonLocalAlignment2021,wangCAIBCCapturingAllround2022}{{2}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{wangViTAAVisualTextualAttributes2020,dingSemanticallySelfAlignedNetwork2021}{{2}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{jiangCrossModalImplicitRelation2023,shaoLearningGranularityUnifiedRepresentations2022}{{2}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{tranDisentangledRepresentationLearning2017}{{2}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{yuan2021transformer}{{2}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{charpentierStrongerReidentificationAttacks2025,liuMoAVRMixtureofAgentsSystem2025}{{2}{2.1}{subsection.2.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}\hskip -1em.~Feature Disentanglement}{2}{subsection.2.2}\protected@file@percent }
\@writefile{brf}{\backcite{wangDisentangledRepresentationLearning2024}{{2}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{liuMultitaskAdversarialNetwork2018}{{2}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{sanchezLearningDisentangledRepresentations2019}{{2}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{chengImprovingDisentangledText2020}{{2}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{materzynskaDisentanglingVisualWritten2022}{{2}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{liuMultitaskAdversarialNetwork2018}{{2}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{chengDisentangledFeatureRepresentation2021}{{2}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{ganinUnsupervisedDomainAdaptation2015}{{2}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{materzynskaDisentanglingVisualWritten2022}{{2}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{liDisentanglingIdentityFeatures2024,azadActivityBiometricsPersonIdentification2024}{{2}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{baiDisentangledFeatureLearning2020}{{2}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{Cui}{{2}{2.2}{subsection.2.2}}}
\citation{schmidtRobustCanonicalizationBootstrapped2025}
\citation{Yin}
\citation{Kim}
\citation{Feng}
\citation{Lid}
\citation{jiangModelingThousandsHuman2025}
\citation{hanafiFastDBSCANAlgorithm2022}
\@writefile{brf}{\backcite{liDisentanglingIdentityFeatures2024}{{3}{2.2}{subsection.2.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}\hskip -1em.~Feature Fusion}{3}{subsection.2.3}\protected@file@percent }
\@writefile{brf}{\backcite{schmidtRobustCanonicalizationBootstrapped2025}{{3}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{Yin}{{3}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{Kim}{{3}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{Feng}{{3}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{Lid}{{3}{2.3}{subsection.2.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.~Method}{3}{section.3}\protected@file@percent }
\newlabel{sec:method}{{3}{3}{\hskip -1em.~Method}{section.3}{}}
\newlabel{sec:method@cref}{{[section][3][]3}{[1][3][]3}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.~Overview}{3}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.~MLLM}{3}{subsection.3.2}\protected@file@percent }
\@writefile{brf}{\backcite{jiangModelingThousandsHuman2025}{{3}{3.2}{subsection.3.2}}}
\@writefile{brf}{\backcite{hanafiFastDBSCANAlgorithm2022}{{3}{3.2}{subsection.3.2}}}
\citation{radfordLearningTransferableVisual2021}
\citation{dosovitskiyImageWorth16x162021}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces First, we use an MLLM with predefined prompts to generate identity-related and clothing-related descriptions from the pedestrian image, which are encoded as $f^t_{clo}$ and $f^t_{id}$, respectively. Then, the input pedestrian image is encoded into a visual feature $f_i$, which still resides in an entangled feature space. Subsequently, the BDAM module, composed of a dual-branch attention mechanism, decouples $f_i$ into identity feature $f_{\text  {id}}$ and non-identity feature $f_{\text  {clo}}$. The decoupling process is supervised and optimized through disentanglement loss and corresponding textual descriptions via contrastive learning. Finally, the fusion module integrates $f_{\text  {id}}$ and $f_{\text  {id}}^t$ to generate the final fused feature representation.}}{4}{figure.caption.2}\protected@file@percent }
\newlabel{fig:figure2}{{2}{4}{First, we use an MLLM with predefined prompts to generate identity-related and clothing-related descriptions from the pedestrian image, which are encoded as $f^t_{clo}$ and $f^t_{id}$, respectively. Then, the input pedestrian image is encoded into a visual feature $f_i$, which still resides in an entangled feature space. Subsequently, the BDAM module, composed of a dual-branch attention mechanism, decouples $f_i$ into identity feature $f_{\text {id}}$ and non-identity feature $f_{\text {clo}}$. The decoupling process is supervised and optimized through disentanglement loss and corresponding textual descriptions via contrastive learning. Finally, the fusion module integrates $f_{\text {id}}$ and $f_{\text {id}}^t$ to generate the final fused feature representation}{figure.caption.2}{}}
\newlabel{fig:figure2@cref}{{[figure][2][]2}{[1][3][]4}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.~Bidirectional Decoupled Alignment Module}{4}{subsection.3.3}\protected@file@percent }
\@writefile{brf}{\backcite{radfordLearningTransferableVisual2021}{{4}{3.3}{subsection.3.3}}}
\@writefile{brf}{\backcite{dosovitskiyImageWorth16x162021}{{4}{3.3}{subsection.3.3}}}
\newlabel{eq:adv}{{1}{5}{\hskip -1em.~Bidirectional Decoupled Alignment Module}{equation.1}{}}
\newlabel{eq:adv@cref}{{[equation][1][]1}{[1][5][]5}{}{}{}}
\newlabel{eq:decouple}{{2}{5}{\hskip -1em.~Bidirectional Decoupled Alignment Module}{equation.2}{}}
\newlabel{eq:decouple@cref}{{[equation][2][]2}{[1][5][]5}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}\hskip -1em.~Feature Fusion}{5}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}\hskip -1em.~Loss Function}{5}{subsection.3.5}\protected@file@percent }
\newlabel{eq:info}{{3}{5}{\hskip -1em.~Loss Function}{equation.3}{}}
\newlabel{eq:info@cref}{{[equation][3][]3}{[1][5][]5}{}{}{}}
\citation{Lie}
\citation{Zhu}
\citation{dingSemanticallySelfAlignedNetwork2021}
\newlabel{eq:triplet}{{4}{6}{\hskip -1em.~Loss Function}{equation.4}{}}
\newlabel{eq:triplet@cref}{{[equation][4][]4}{[1][5][]6}{}{}{}}
\newlabel{eq:gradnorm}{{5}{6}{\hskip -1em.~Loss Function}{equation.5}{}}
\newlabel{eq:gradnorm@cref}{{[equation][5][]5}{[1][6][]6}{}{}{}}
\newlabel{eq:total}{{6}{6}{\hskip -1em.~Loss Function}{equation.6}{}}
\newlabel{eq:total@cref}{{[equation][6][]6}{[1][6][]6}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.~Experiments}{6}{section.4}\protected@file@percent }
\newlabel{sec:experiments}{{4}{6}{\hskip -1em.~Experiments}{section.4}{}}
\newlabel{sec:experiments@cref}{{[section][4][]4}{[1][6][]6}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\hskip -1em.~Datasets and Metrics}{6}{subsection.4.1}\protected@file@percent }
\@writefile{brf}{\backcite{Lie}{{6}{4.1}{subsection.4.1}}}
\@writefile{brf}{\backcite{Zhu}{{6}{4.1}{subsection.4.1}}}
\@writefile{brf}{\backcite{dingSemanticallySelfAlignedNetwork2021}{{6}{4.1}{subsection.4.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}\hskip -1em.~Implementation Details}{6}{subsection.4.2}\protected@file@percent }
\newlabel{sec:4.2}{{4.2}{6}{\hskip -1em.~Implementation Details}{subsection.4.2}{}}
\newlabel{sec:4.2@cref}{{[subsection][2][4]4.2}{[1][6][]6}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}\hskip -1em.~Ablation Study}{6}{subsection.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces End to end model enhancement.}}{7}{table.caption.3}\protected@file@percent }
\newlabel{tab:enhancement_fit}{{1}{7}{End to end model enhancement}{table.caption.3}{}}
\newlabel{tab:enhancement_fit@cref}{{[table][1][]1}{[1][6][]7}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Ablation on the BDAM module.}}{7}{table.caption.4}\protected@file@percent }
\newlabel{tab:ablation_bdam}{{2}{7}{Ablation on the BDAM module}{table.caption.4}{}}
\newlabel{tab:ablation_bdam@cref}{{[table][2][]2}{[1][6][]7}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces t-SNE visualization comparison. (a) Baseline (w/o disentanglement) produces a highly entangled 2D point cloud with heavy identity overlap, indicating identity窶田lothing coupling. (b) With BDAM + HSIC, embeddings form tighter same-identity clusters, clearer inter-identity boundaries, and a center-to-periphery color gradient reflecting progressive identity窶田lothing disentanglement.}}{7}{figure.caption.5}\protected@file@percent }
\newlabel{fig:demo}{{3}{7}{t-SNE visualization comparison. (a) Baseline (w/o disentanglement) produces a highly entangled 2D point cloud with heavy identity overlap, indicating identity窶田lothing coupling. (b) With BDAM + HSIC, embeddings form tighter same-identity clusters, clearer inter-identity boundaries, and a center-to-periphery color gradient reflecting progressive identity窶田lothing disentanglement}{figure.caption.5}{}}
\newlabel{fig:demo@cref}{{[figure][3][]3}{[1][7][]7}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Ablation on Fusion Module.}}{7}{table.caption.6}\protected@file@percent }
\newlabel{tab:ablation_fusion}{{3}{7}{Ablation on Fusion Module}{table.caption.6}{}}
\newlabel{tab:ablation_fusion@cref}{{[table][3][]3}{[1][7][]7}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Ablation on Individual Loss Components.}}{7}{table.caption.7}\protected@file@percent }
\newlabel{tab:ablation_loss}{{4}{7}{Ablation on Individual Loss Components}{table.caption.7}{}}
\newlabel{tab:ablation_loss@cref}{{[table][4][]4}{[1][7][]7}{}{}{}}
\citation{jiangCrossModalImplicitRelation2023}
\citation{Liub}
\citation{Zuoa}
\citation{Yan}
\citation{Qin}
\citation{jiangModelingThousandsHuman2025}
\citation{Zheng}
\citation{liPromptDecouplingTexttoImage2024}
\citation{dingSemanticallySelfAlignedNetwork2021}
\citation{Yana}
\citation{Shu}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Ablation on style prompts.}}{8}{table.caption.8}\protected@file@percent }
\newlabel{tab:ablation_style_prompts}{{5}{8}{Ablation on style prompts}{table.caption.8}{}}
\newlabel{tab:ablation_style_prompts@cref}{{[table][5][]5}{[1][7][]8}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}\hskip -1em.~Efficiency Analysis}{8}{subsection.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Comparative analysis of model efficiency and performance on CUHK-PEDES. Each subplot presents a key metric, enabling direct trade-off comparisons. Model abbreviations: Simple (S), Transformer-2Layer (T(2)), Performer (P), Transformer-4Layer (T(4)), and Ours.}}{8}{figure.caption.9}\protected@file@percent }
\newlabel{fig:efficiency_comparison}{{4}{8}{Comparative analysis of model efficiency and performance on CUHK-PEDES. Each subplot presents a key metric, enabling direct trade-off comparisons. Model abbreviations: Simple (S), Transformer-2Layer (T(2)), Performer (P), Transformer-4Layer (T(4)), and Ours}{figure.caption.9}{}}
\newlabel{fig:efficiency_comparison@cref}{{[figure][4][]4}{[1][8][]8}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}\hskip -1em.~Comparisons with State-of-the-Art Methods}{8}{subsection.4.5}\protected@file@percent }
\bibstyle{ieee_fullname}
\bibdata{egbib}
\bibcite{azadActivityBiometricsPersonIdentification2024}{1}
\bibcite{baiDisentangledFeatureLearning2020}{2}
\bibcite{bukhariLanguageVisionBased2023}{3}
\bibcite{charpentierStrongerReidentificationAttacks2025}{4}
\bibcite{chengDisentangledFeatureRepresentation2021}{5}
\bibcite{chengImprovingDisentangledText2020}{6}
\bibcite{Cui}{7}
\bibcite{dingSemanticallySelfAlignedNetwork2021}{8}
\bibcite{dosovitskiyImageWorth16x162021}{9}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Comparisons with state-of-the-art ReID methods under the traditional evaluation setting.}}{9}{table.caption.10}\protected@file@percent }
\newlabel{tab:sota_comparison}{{6}{9}{Comparisons with state-of-the-art ReID methods under the traditional evaluation setting}{table.caption.10}{}}
\newlabel{tab:sota_comparison@cref}{{[table][6][]6}{[1][8][]9}{}{}{}}
\@writefile{brf}{\backcite{jiangCrossModalImplicitRelation2023}{{9}{6}{table.caption.10}}}
\@writefile{brf}{\backcite{Liub}{{9}{6}{table.caption.10}}}
\@writefile{brf}{\backcite{Zuoa}{{9}{6}{table.caption.10}}}
\@writefile{brf}{\backcite{Yan}{{9}{6}{table.caption.10}}}
\@writefile{brf}{\backcite{Qin}{{9}{6}{table.caption.10}}}
\@writefile{brf}{\backcite{jiangModelingThousandsHuman2025}{{9}{6}{table.caption.10}}}
\@writefile{brf}{\backcite{Zheng}{{9}{6}{table.caption.10}}}
\@writefile{brf}{\backcite{liPromptDecouplingTexttoImage2024}{{9}{6}{table.caption.10}}}
\@writefile{brf}{\backcite{dingSemanticallySelfAlignedNetwork2021}{{9}{6}{table.caption.10}}}
\@writefile{brf}{\backcite{Yana}{{9}{6}{table.caption.10}}}
\@writefile{brf}{\backcite{Shu}{{9}{6}{table.caption.10}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.~Conclusion and Limitations}{9}{section.5}\protected@file@percent }
\newlabel{sec:conclusion}{{5}{9}{\hskip -1em.~Conclusion and Limitations}{section.5}{}}
\newlabel{sec:conclusion@cref}{{[section][5][]5}{[1][9][]9}{}{}{}}
\bibcite{Feng}{10}
\bibcite{galiyawalaPersonRetrievalSurveillance2021}{11}
\bibcite{ganinUnsupervisedDomainAdaptation2015}{12}
\bibcite{gaoContextualNonLocalAlignment2021}{13}
\bibcite{hanafiFastDBSCANAlgorithm2022}{14}
\bibcite{heInstructReIDUniversalPurpose2025}{15}
\bibcite{jiangCrossModalImplicitRelation2023}{16}
\bibcite{jiangModelingThousandsHuman2025}{17}
\bibcite{Kim}{18}
\bibcite{Lid}{19}
\bibcite{liPersonSearchNatural2017}{20}
\bibcite{Lie}{21}
\bibcite{liPromptDecouplingTexttoImage2024}{22}
\bibcite{liDisentanglingIdentityFeatures2024}{23}
\bibcite{liuMoAVRMixtureofAgentsSystem2025}{24}
\bibcite{Liub}{25}
\bibcite{liuMultitaskAdversarialNetwork2018}{26}
\bibcite{materzynskaDisentanglingVisualWritten2022}{27}
\bibcite{niuLLMLocBootstrapSingleimage2025}{28}
\bibcite{Qin}{29}
\bibcite{radfordLearningTransferableVisual2021}{30}
\bibcite{sanchezLearningDisentangledRepresentations2019}{31}
\bibcite{schmidtRobustCanonicalizationBootstrapped2025}{32}
\bibcite{shaoLearningGranularityUnifiedRepresentations2022}{33}
\bibcite{Shu}{34}
\bibcite{tanHarnessingPowerMLLMs2024}{35}
\bibcite{tranDisentangledRepresentationLearning2017}{36}
\bibcite{wangLearningDeepStructurePreserving2016}{37}
\bibcite{wangDisentangledRepresentationLearning2024}{38}
\bibcite{wangLanguagePersonSearch2019}{39}
\bibcite{wangViTAAVisualTextualAttributes2020}{40}
\bibcite{wangCAIBCCapturingAllround2022}{41}
\bibcite{yanLearningComprehensiveRepresentations2023}{42}
\bibcite{Yana}{43}
\bibcite{Yan}{44}
\bibcite{yangUnifiedTextbasedPerson2023}{45}
\bibcite{yaoFILIPFinegrainedInteractive2021}{46}
\bibcite{Yin}{47}
\bibcite{yuan2021transformer}{48}
\bibcite{zhaoLuoJiaHOGHierarchyOriented2024}{49}
\bibcite{Zheng}{50}
\bibcite{Zhu}{51}
\bibcite{zuoPLIPLanguageImagePretraining2024}{52}
\bibcite{Zuoa}{53}
\gdef \@abspage@last{11}
