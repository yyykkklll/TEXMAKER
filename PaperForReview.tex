\documentclass[10pt,twocolumn,letterpaper]{article}

% --- 基础宏包 ---
\usepackage{cvpr}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{float}
\usepackage{hhline}
\usepackage{caption}
\usepackage[activate={true,nocompatibility},final,tracking=true,kerning=true,stretch=10,shrink=10]{microtype}

% --- 表格与链接宏包 ---
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}
\usepackage{tabularray}
\usepackage[table]{xcolor}
\usepackage{colortbl}
\usepackage{stfloats}

% --- 交叉引用宏包 ---
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% --- 页面布局与会议信息 ---
\setlength{\parskip}{0pt}
\setlength{\parindent}{2em}
\def\cvprPaperID{*****} % 输入CVPR论文ID
\def\confName{CVPR}
\def\confYear{2022}
\linespread{1.05}

\begin{document}

% --- 自定义命令 ---
\newcommand{\thickhline}{%
    \noalign{\global\dimen1=\arrayrulewidth\global\arrayrulewidth=1pt}%
    \hline
    \noalign{\global\arrayrulewidth=\dimen1}%
}
\newcommand{\thickhhline}[1]{%
    \noalign{\global\dimen1=\arrayrulewidth\global\arrayrulewidth=1pt}%
    \hhline{#1}%
    \noalign{\global\arrayrulewidth=\dimen1}%
}

% --- 标题与作者 ---
\title{Robust Person Re-Identification via MLLM-Supervised Feature Decoupling}
\author{
First Author\\
Institution1\\
Institution1 Address\\
{\tt\small firstauthor@i1.org}
\and
Second Author\\
Institution2\\
Institution2 Address\\
{\tt\small secondauthor@i2.org}
}
\maketitle

% --- 摘要 ---
\begin{abstract}
    \noindent The challenge of fine-grained semantic alignment continues to hinder Text-to-Image Person Re-Identification, where clothing-induced interference and a persistent modality gap degrade retrieval accuracy. In this paper, we propose a novel framework to resolve this issue, centered on MLLM-supervised feature decoupling. Our framework introduces two core components: a Bidirectional Decoupling Alignment Module and a Mamba State Space Model for efficient fusion. To obtain high-quality, fine-grained supervision, we first employ a Multimodal Large Language Model to automatically generate separate identity and clothing descriptions. These descriptions then guide our decoupling module, which utilizes bidirectional attention and a gated weighting strategy to meticulously disentangle visual features into identity and clothing subspaces. To enforce this separation and ensure identity purity, we design a multi-task loss strategy comprising an adversarial loss that actively suppresses the influence of clothing-related features, and a kernel-based orthogonal constraint that ensures statistical independence. Furthermore, we are the first to integrate the Mamba State Space Model into cross-modal Re-ID as an efficient fusion module. By leveraging its linear-time complexity and proficiency in modeling long-range dependencies, it facilitates deep contextual interactions across modalities while avoiding the quadratic complexity of Transformers. Comprehensive experiments on multiple benchmark datasets reveal that our proposed method achieves superior performance compared to leading contemporary methods, proving its effectiveness and robustness.
\end{abstract}

% --- 正文 ---
\section{Introduction}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{Figure1}
    \caption{Comparison of ReID methods.(a) Traditional: Direct fusion of image and text features without distinguishing identity from non-identity information limits alignment.(b) Proposed (BDAM): Introduces a decoupling module that aligns separated identity/clothing features with MLLM-generated descriptions for fine-grained matching.}
    \label{fig:figure1}
\end{figure}
\label{sec:intro}
\noindent Text-to-image person re-identification (T2I-ReID) retrieves a target pedestrian from a large-scale image gallery given a natural-language description~\cite{jiangCrossModalImplicitRelation2023, yanLearningComprehensiveRepresentations2023,shaoLearningGranularityUnifiedRepresentations2022, dingSemanticallySelfAlignedNetwork2021}.
It is valuable for video surveillance~\cite{bukhariLanguageVisionBased2023}, intelligent security~\cite{galiyawalaPersonRetrievalSurveillance2021}, public safety, and social media.
Despite recent progress, practical deployment remains challenging: image factors (pose, viewpoint, illumination) obscure identity-relevant cues, and a persistent modality gap hampers effective fusion in a shared space.These issues are exacerbated at the fine-grained level, where semantic alignment is particularly difficult. Most recent methods adopt ViT as the image encoder and combine it with multimodal strategies, such as interactive attention, local semantic alignment, and global feature enhancement, to strengthen identity modeling~\cite{dosovitskiyImageWorth16x162021,tanHarnessingPowerMLLMs2024,zuoPLIPLanguageImagePretraining2024,yangUnifiedTextbasedPerson2023,heInstructReIDUniversalPurpose2025,liPromptDecouplingTexttoImage2024}.
Others leverage CLIP to benefit from large-scale contrastive pretraining and well-aligned embeddings~\cite{radfordLearningTransferableVisual2021,yaoFILIPFinegrainedInteractive2021,niuLLMLocBootstrapSingleimage2025,zhaoLuoJiaHOGHierarchyOriented2024}.
However, both lines commonly treat the description holistically, overlooking the distinction between identity-relevant (e.g., gender, body shape) and identity-irrelevant (e.g., clothing, hairstyle) content.
In complex scenes with background clutter, large clothing variations, or redundant details, this coarse treatment blurs identity cues, weakens feature decoupling, and degrades cross-modal matching robustness.
To address this, we draw on the style-clustering paradigm of HAM~\cite{jiangModelingThousandsHuman2025} and adapt it to identity–clothing decoupling.
A pre-trained visual encoder extracts global identity cues and local clothing details.
Guided by prompts, an MLLM generates two distinct texts for each pedestrian: an identity description (biological traits such as race, gender, age, body type) and a clothing description (apparel type, color, pattern, material, accessories).
We then apply DBSCAN to cluster identity and clothing feature spaces, with noise reallocation and small-cluster merging to improve stability.
A dual-prompt generator governs output style: identity texts prioritize accuracy, whereas clothing texts encourage diversity;
syntax checks and attribute validation further ensure quality.

Building on these decoupled annotations, we propose a Bidirectional Decoupling Alignment Module (BDAM) to explicitly separate and align identity and clothing information;
see \cref{fig:figure2}. Instead of fusing features from independent encoders without discrimination, BDAM exploits the generated identity and clothing texts for contrastive supervision.
Concretely, it takes the full patch sequence from the image encoder, linearly projects tokens into identity and clothing subspaces, applies a dual-branch self-attention to refine intra-subspace semantics, and performs multi-layer cross-attention for bidirectional interaction to improve inter-subspace modeling and decoupling precision.
A gated weighting mechanism adaptively balances identity and clothing contributions.
To enforce subspace independence, we introduce an orthogonality constraint based on HSIC, which drives the two feature types into mutually independent subspaces and strengthens decoupling.
Beyond decoupling, we further employ Mamba SSM as a fusion module to capture long-range cross-modal dependencies with linear complexity, enhancing alignment while mitigating Transformer efficiency bottlenecks.
The main contributions are: (1) A prompt auto-construction pipeline that combines style clustering with an MLLM to produce fine-grained, diverse, and decoupled identity and clothing descriptions, enriching supervision and discriminability.
(2) BDAM, which achieves efficient decoupling and precise alignment of identity-relevant features via bidirectional contrastive learning with the MLLM-generated descriptions, reinforced by a multi-task loss strategy combining an adversarial loss and an HSIC-based orthogonality constraint.
(3) A Mamba SSM fusion module that models long-range cross-modal dependencies with linear complexity, improving semantic correlation capture and alignment robustness.
\section{Related Work}
\subsection{Text-to-Image Re-ID}
\noindent A core challenge in T2I-ReID is the semantic gap between images and text.
Early work attempted to reduce this discrepancy by projecting global visual and textual features into a shared space~\cite{wangLearningDeepStructurePreserving2016,wangLanguagePersonSearch2019,liPersonSearchNatural2017}, but high intra-class variance and low inter-class variance across both modalities hinder reliable cross-modal matching.
To overcome this, subsequent studies introduced feature disentanglement, broadly via explicit or implicit alignment~\cite{gaoContextualNonLocalAlignment2021,wangCAIBCCapturingAllround2022}.
Explicit methods~\cite{wangViTAAVisualTextualAttributes2020,dingSemanticallySelfAlignedNetwork2021} detect body parts or attributes and align local regions and phrases with auxiliary modules.
Implicit methods~\cite{jiangCrossModalImplicitRelation2023,shaoLearningGranularityUnifiedRepresentations2022} avoid external tools and use regularizers to associate noun phrases with image regions.
Representative examples include adversarial objectives that force identity and non-identity features to compete in feature space~\cite{tranDisentangledRepresentationLearning2017} and multi-head attention to accentuate semantic separation~\cite{yuan2021transformer}.
Although these strategies improve in-domain performance, generalization to unseen domains is often limited by the small scale of manual annotations. In general, global alignment alone is fragile in complex scenes~\cite{charpentierStrongerReidentificationAttacks2025,liuMoAVRMixtureofAgentsSystem2025}. Distinguishing identity-relevant from identity-irrelevant semantics is essential;
therefore, disentanglement has become a key avenue to advance T2I-ReID.
\subsection{Feature Disentanglement}
\noindent Feature disentanglement~\cite{wangDisentangledRepresentationLearning2024} aims to separate semantically distinct factors in feature space to improve interpret ability and generalization.
Early approaches often leveraged generative models (VAEs, GANs)\cite{liuMultitaskAdversarialNetwork2018} to partition latent codes into structured factors.
More recently, disentanglement has expanded beyond generation to image classification\cite{sanchezLearningDisentangledRepresentations2019}, NLP~\cite{chengImprovingDisentangledText2020}, and multimodal learning~\cite{materzynskaDisentanglingVisualWritten2022}.
Mainstream strategies include min–max multi-task adversarial training that jointly optimizes an encoder and a style discriminator~\cite{liuMultitaskAdversarialNetwork2018}; metric-learning schemes such as DFR~\cite{chengDisentangledFeatureRepresentation2021} with Gradient Reversal Layer~\cite{ganinUnsupervisedDomainAdaptation2015} to decorrelate factors;
and orthogonal linear projections that separate visual and textual embeddings under CLIP~\cite{materzynskaDisentanglingVisualWritten2022}.
In Re-ID, disentanglement typically separates identity-relevant signals from nuisances \cite{liDisentanglingIdentityFeatures2024,azadActivityBiometricsPersonIdentification2024}. Generative designs model these factors independently;
in vehicle Re-ID, DFLNet\cite{baiDisentangledFeatureLearning2020} jointly extracts orientation-specific and generic features. For occlusion, ProFD~\cite{Cui} uses text prompts to isolate body-part features.
Clothing-changing Re-ID often adopts dual-stream architectures to counter appearance shifts and camera bias~\cite{liDisentanglingIdentityFeatures2024}.
These efforts improve semantic purity and factor independence, yet two limitations persist.
First, without an effective interaction mechanism, isolated factors may fail to support robust cross-modal matching, leading to brittle alignment.
Second, reliance on manual annotations or external detectors constrains scalability and domain transfer, and implicit regularizers can be underconstrained, yielding spurious separations on unseen data.
Consequently, recent work emphasizes coupling disentanglement with principled interactions and independence constraints.
In this spirit, our framework pairs data-side decoupling with model-side disentanglement and independence enforcement, providing explicit supervision and controllable separation while preserving cross-modal synergy.
\subsection{Feature Fusion}
\noindent Feature fusion is central to T2I-ReID, with most methods relying on Transformers or CLIP~\cite{schmidtRobustCanonicalizationBootstrapped2025}.
Cross-modal modules built on multi-head attention process image–text tokens in parallel to capture semantic associations~\cite{Yin}, but their quadratic complexity in sequence length causes memory and latency spikes for high-resolution images or long descriptions.
CLIP-based pipelines~\cite{Kim} benefit from large-scale contrastive pretraining and well-aligned embeddings, yet global pooling and holistic processing often blur identity versus clothing cues, leading to semantic confusion under clothing changes or verbose descriptions.
Dynamic fusion that reweights modalities via attention can improve adaptivity~\cite{Feng}, but introduces higher computational cost, tuning sensitivity, and performance instability across datasets.
Graph-based fusion~\cite{Lid} models dependencies through GNNs, offering relational inductive bias, whereas assumptions about graph structure and stationarity limit adaptability to free-form, variable-length text and dynamic visual contexts.
Contemporary evidence suggests that accuracy and robustness improve only when semantic disentanglement and efficient fusion advance in tandem.
Practically, fusion should (i) respect factorized semantics (e.g., identity/clothing) to avoid re-coupling nuisances, (ii) capture long-range cross-modal dependencies, and (iii) scale with linear or near-linear complexity to handle long sequences and high-res tokens.
This motivates our design: BDAM supplies factor-aware representations and decoupled supervision, while a Mamba SSM fusion module models long-horizon interactions with linear complexity, enabling precise alignment without the memory and efficiency bottlenecks of standard Transformers.
\section{Method}
\label{sec:method}
\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figure2_2.pdf}
    \caption{First, we use an MLLM with predefined prompts to generate identity-related and clothing-related descriptions from the pedestrian image, which are encoded as $f^t_{clo}$ and $f^t_{id}$, respectively.
    Then, the input pedestrian image is encoded into a visual feature $f_i$, which still resides in an entangled feature space.
    Subsequently, the BDAM module, composed of a dual-branch attention mechanism, decouples $f_i$ into identity feature $f_{\text{id}}$ and non-identity feature $f_{\text{clo}}$.
    The decoupling process is supervised and optimized through disentanglement loss and corresponding textual descriptions via contrastive learning.
    Finally, the fusion module integrates $f_{\text{id}}$ and $f_{\text{id}}^t$ to generate the final fused feature representation.}
    \label{fig:figure2}
\end{figure*}

\subsection{Overview}
\noindent To learn pedestrian representations robust to variations in clothing, pose, and environment, this paper proposes the BDAM.
This module disentangles and extracts robust features via contrastive and supervised learning between the input image and encoded textual features.
As illustrated in \cref{fig:figure2}, BDAM improves T2I-ReID retrieval performance by effectively separating identity from clothing features, thereby enabling superior cross-modal fusion.
The core objective is to learn an identity representation invariant to clothing changes.
To achieve this, our model comprises four key components: (1) a Visual Feature Extraction Module, (2) a Textual Feature Extraction Module, (3) the BDAM, and (4) a Mamba SSM Fusion Module.
Specifically, given a pedestrian image $I \in \mathbb{R}^{B \times C \times H \times W}$, a visual encoder first extracts image features $f_i$.
To obtain semantic guidance, we use an MLLM with pre-designed prompts to generate corresponding identity-relevant and identity-irrelevant textual descriptions.
A text encoder subsequently encodes these into $f^t_{id}$ and $f^t_{clo}$.
During disentanglement, BDAM leverages these textual features to guide image feature learning.
To ensure the quality of this process, we introduce an HSIC loss to constrain the two resulting feature types towards orthogonality.
We also employ an adversarial clothing loss, denoted as $\mathcal{L}_{\text{adv}}$, to actively suppress clothing-related features and ensure they do not interfere with identity recognition.
Finally, to achieve deep fusion of visual identity and textual semantics, we introduce the Mamba SSM as a fusion module.
It dynamically models and facilitates interaction among the disentangled multi-modal features, enhancing the model's overall representation capability.
\subsection{MLLM}
\noindent We employ an MLLM(specifically, ChatGPT-4o as detailed in ~\ref{sec:4.2}) to automatically generate fine-grained identity and clothing descriptions for pedestrians, reducing manual annotation and enriching supervision.
Prior work, notably HAM~\cite{jiangModelingThousandsHuman2025}, shows that modeling annotator styles can steer an MLLM to produce diverse texts.
Building on our reproduction of HAM, we adapt and extend the pipeline to meet our model's design goals.

We use the CLIP text encoder (ViT-L/14) to embed the original descriptions into fixed-dimensional vectors.With prompts, an MLLM generalizes and substitutes entity attributes to emphasize expression style rather than content.
We then cluster these style embeddings with DBSCAN~\cite{hanafiFastDBSCANAlgorithm2022}, which adaptively discovers dense regions without predefining the cluster count.To stabilize clusters, we reassign noise points and merge small clusters.
This setup aligns the learned style categories with the identity/clothing disentanglement expected by BDAM.

A dual-prompt generator guides the MLLM to output two texts per image: an identity description (biological traits such as race, gender, age, body type) and a clothing description (apparel type, color, pattern, material, accessories).
We control generation with length and temperature and apply syntax checks plus attribute-coverage validation so that outputs remain grammatical, structured, and parsable.
Summary. Our adaptations deliver flexible style modeling via DBSCAN and a decoupled dual-description mechanism that strengthens the distinctiveness and diversity of identity and clothing semantics.
The resulting supervision improves data expressiveness and provides richer training signals for BDAM.
\subsection{Bidirectional Decoupled Alignment Module}
\noindent Some studies directly adopt CLIP~\cite{radfordLearningTransferableVisual2021} as a dual-modality feature extractor and align global embeddings for retrieval or discrimination.
However, two limitations arise: (i) limited fine-grained semantics hinder separating identity from clothing, and (ii) holistic image–text encoding lacks token-level cross-modal structure modeling, reducing robustness in complex scenes.
In this paper, we use a pre-trained ViT (ViT-B/16) as the visual encoder $E_v$~\cite{dosovitskiyImageWorth16x162021}.
Given an image $I_i$, $E_v$ outputs token features $f_i \in \mathbb{R}^{B \times L \times D}$ that entangle identity-relevant and identity-irrelevant cues.
A dual-branch linear projection yields preliminary identity features $f_{\text{id}}' \in \mathbb{R}^{B \times L \times D}$, and clothing features $f_{\text{clo}}' \in \mathbb{R}^{B \times L \times D}$, followed by multi-layer self-attention in each branch to enhance local consistency and contextual awareness.
Instead of using the ViT [CLS] token as a global descriptor, we exploit the full patch sequence and introduce cross-branch cross-attention to exchange information.
In the identity branch, the clothing branch provides auxiliary context, and vice versa, reinforcing semantic distinctions.
Each branch then applies global average pooling to produce $\hat{f}{id}$ and $\hat{f}{clo}$.
To enable soft, input-adaptive disentanglement, we design a gating mechanism.
The two global vectors are concatenated and fed to a lightweight linear network with a Sigmoid output, producing a gate $g \in \mathbb{R}^{B \times D}$.
We obtain gated features $f_{id}^{gate} = g \odot \hat{f}_{id}$ and $f_{clo}^{gate} = (1-g) \odot \hat{f}_{clo}$, where $\odot$ denotes element-wise multiplication.
This dimension-wise weighting provides fine-grained control; $f_{id}^{gate}$ is further sent to the fusion module.
To train BDAM and enforce separation, we introduce a clothing adversarial loss and an HSIC-based identity–clothing decoupling loss.
The clothing adversarial loss suppresses correlations between visual clothing features and clothing text, enhancing the purity of identity features:
\begin{equation} \label{eq:adv} \mathcal{L}_{\text{adv}} = \mathbb{E}_i \left[ -\log(1 - P_{\text{pos}}(i)) \right] \end{equation}
where $P_{\text{pos}}(i)$ is the temperature-scaled dot-product similarity between $\hat{f}_{\text{clo}}$ and $f_{\text{clo}}^t$.
In practice, clothing features are linearly projected to the text dimension and L2-normalized for stable similarity estimation.
This adversarial objective explicitly downweights clothing, while cross-attention implicitly sharpens identity/clothing separation via interaction.
By minimizing the matching probability between the visual clothing features and the text, $\mathcal{L}_{\text{adv}}$ encourages the model to weaken the distracting effect of clothing features on identity recognition.
This adversarial mechanism and the cross-attention mechanism are complementary. The former explicitly reduces the weight of clothing information by optimizing an objective function, while the latter strengthens the semantic differences between identity and clothing features through interactive modeling.
To further encourage statistical independence, we minimize an HSIC-based decoupling loss:
\begin{equation}
    \label{eq:decouple}
    \begin{split}
        \mathcal{L}_{\text{Decouple}} & = \text{HSIC}(f_{\text{id}}^{\text{gate}}, f_{\text{clo}}^{\text{gate}}) \\
                                      & = \frac{1}{(N-1)^2} \mathrm{tr}(K_{\text{id}} H K_{\text{clo}} H)
    \end{split}
\end{equation}
Here, $f_{\text{id}}^{\text{gate}} \in \mathbb{R}^{B \times D}$ and $f_{\text{clo}}^{\text{gate}} \in \mathbb{R}^{B \times D}$ are the gated identity and clothing features, respectively.
$K_{\text{id}} = f_{\text{id}}^{\text{gate}} (f_{\text{id}}^{\text{gate}})^{T}$ and $K_{\text{clo}} = f_{\text{clo}}^{\text{gate}} (f_{\text{clo}}^{\text{gate}})^{T}$ are their respective kernel matrices.
$H = I_{N} - (1/N) \mathbf{1}_{N} \mathbf{1}_{N}^{T}$ is the centering matrix, where $I_{N}$ is the $N$-dimensional identity matrix and $\mathbf{1}_{N}$ is a column vector of all ones.
HSIC measures the statistical dependence between features by calculating the mean trace of the product of their kernel matrices and the centering matrix.
By minimizing this value, the loss encourages the features to be statistically independent.

\subsection{Feature Fusion}
\noindent For efficient and semantically-sensitive cross-modal feature fusion, we introduce the Mamba SSM.The core objective is to preserve the semantic integrity of the purified image and text identity features, enabling a fine-grained fusion that is robust to the clothing variations previously isolated by the BDAM.The module first aligns features from both modalities, then applies a pre-fusion gating mechanism to dynamically adjust their weights, and finally employs a multi-layer Mamba SSM for deep semantic interaction to generate the final fused representation.Initially, to mitigate distributional discrepancies between modalities, MLP performs dimensional alignment.It processes the decoupled visual features, $f_{\text{id}}^{\text{gate}}$, and the textual features, $f_{\text{id}}^{t}$, to generate aligned features, $f_{\text{img}}$ and $f_{\text{txt}}$.Notably, the decoupled visual clothing feature, $f_{\text{clo}}^{\text{gate}}$, is \textit{intentionally discarded} during fusion.This design is central to our goal: the BDAM, supervised by $\mathcal{L}_{\text{adv}}$ and $\mathcal{L}_{\text{Decouple}}$, is tasked with purging identity-irrelevant information into $f_{\text{clo}}^{\text{gate}}$.By excluding this feature from the final fusion, the model is forced to learn a representation based purely on stable identity semantics, thereby achieving robustness against clothing changes.Following this, a gating mechanism achieves dynamic weighted fusion. It outputs a weight vector $w \in \mathbb{R}^{B \times 2}$, which is normalized via a SoftMax layer to produce image $w_{1}$ and text $w_{2}$, satisfying $w_{1} + w_{2} = 1$.The resulting fusion is computed as: $f_{\text{fusion}} = w_{1} \cdot f_{\text{img}} + w_{2} \cdot f_{\text{txt}}$.This allows the model to adaptively balance modal contributions based on context.This fusion gate is distinct from the one in the disentanglement module.It outputs a global, two-dimensional weight vector $w \in \mathbb{R}^{B \times 2}$, for the modalities, whereas the disentanglement gate provides a dimension-wise weight vector $g \in \mathbb{R}^{B \times D}$ for fine-grained feature control.The fused features, $f_{fusion}$, are then processed by the Mamba SSM to enhance inter-modal interaction.Leveraging its long-range dependency modeling capabilities, Mamba effectively captures complex sequential relationships between the modalities.We employ a stack of Mamba layers, where each layer updates its input $f_{fusion}^{(l)}$ using a residual connection: $f_{fusion}^{(l+1)} = \text{Mamba}(f_{fusion}^{(l)}) + f_{fusion}^{(l)}$.This structure mitigates the vanishing gradient problem and improves information flow.Finally, the output from the last Mamba layer is projected to produce the final representation, $f_{final} \in \mathbb{R}^{B \times D_{out}}$.The resulting feature is highly adaptive in its modal weighting and benefits from Mamba's long-range semantic modeling,providing robust support for downstream tasks like person re-identification.

\subsection{Loss Function}
\noindent To achieve fine-grained cross-modal alignment, we adopt the InfoNCE loss, which maximizes similarity for positive image–text pairs (same identity) while separating negatives.It is defined as:
\begin{equation} \label{eq:info} \mathcal{L}{_\text{info}} = -\log \frac{\exp(\mathbf{v}^\top \mathbf{t}i / \tau)}{\sum_j \exp(\mathbf{v}^\top \mathbf{t}j / \tau)} \end{equation}
Here, $v_i$ is the L2-normalized image identity feature from the visual encoder, $t_i$ is the text feature for the $i$-th identity, and $\tau$ controls distribution sharpness.In-batch negatives reduce the cross-modal semantic gap and promote alignment in a shared space.To enhance intra-modal identity discrimination, we include a triplet loss that enforces intra-class compactness and inter-class separation:
\begin{equation} \label{eq:triplet} \mathcal{L}{_\text{triplet}} = \mathbb{E}{(a,p,n)} \left[ \max\left( | f_a - f_p |2^2 - | f_a - f_n |2^2 + m, 0 \right) \right] \end{equation}
Here, $(a, p, n)$ represent the identity features of the anchor, positive, and negative samples, respectively;$||\cdot||_2$ denotes the L2 norm; and $m$ is the margin parameter used to enforce a minimum distance gap between positive and negative pairs.In multi-task training, differing loss scales can cause one task to dominate.We adopt GradNorm to balance training by dynamically adjusting each task's gradient norm:
\begin{equation} \label{eq:gradnorm} \mathcal{L}{_\text{GradNorm}} = \sum_k |    \nabla{\theta} (w_k \mathcal{L}_k) - \tilde{r}k G{\text{ref}} |1 \end{equation}
Here, $\nabla{\theta} (w_k \mathcal{L}k)$ represents the gradient of the weighted loss of the $k$-th task, $w_k \mathcal{L}_k$, with respect to the shared parameters $\theta$;$||\cdot||_1$ denotes the L1 norm, which emphasizes a linear penalty on the deviation;$\tilde{r}_k = (\mathcal{L}_k / \mathcal{L}_k^0) / \bar{r}$ is the normalized loss ratio, where $\mathcal{L}_k^0$ is the initial loss of task $k$ at the beginning of training (used as a baseline), and $\bar{r}$ is the average of the loss ratios over all tasks.$G{ref}$ is a reference gradient norm, typically set to the gradient norm of the first task, $||\nabla{\theta}(w_1\mathcal{L}1)||$, as a baseline.This mechanism enforces a gradient balance among tasks during training by minimizing the L1 deviation between the actual gradient norm and a target value, $\tilde{r}_k G_{ref}$.Furthermore, to prevent instability caused by the abnormal scaling of task weights $w_k$, we add a regularization term, $\lambda \sum_k (\log w_k)^2$, to the total loss.This term effectively suppresses large discrepancies among the task weights by penalizing the square of their logarithms, thereby improving the stability of the training process.Finally, the overall loss function is defined as follows:
\begin{equation}    \label{eq:total}    \mathcal{L}{_\text{Total}} = \sum_k w_k \mathcal{L}_k + \alpha \mathcal{L}{_\text{GradNorm}} + \lambda \sum_k (\log w_k)^2\end{equation}
Here, $\mathcal{L}_k$ represents the loss term for the $k$-th task (which includes $\mathcal{L}{\text{info}}$, $\mathcal{L}_{\text{triplet}}$, $\mathcal{L}_{\text{adv}}$, and $\mathcal{L}_{\text{Decouple}}$), and $w_k = \text{exp}(s_k)$ is the task weight, which is dynamically adjusted by learning the task uncertainty parameter $s_k$.The hyperparameter $\alpha$ is used to control the strength of the GradNorm loss, while $\lambda$ serves as the regularization coefficient.

\section{Experiments}
\label{sec:experiments}
\subsection{Datasets and Metrics}
\noindent \textbf{CUHK-PEDES \cite{Lie}} The first dataset built for T2I-ReID, containing 40,206 images of 13,003 identities, with two text descriptions per image.
We follow the official split: 34,054 images from 11,003 identities for training, 3,078 images from 1,000 identities for validation, and 3,074 images from another 1,000 identities for testing.

\textbf{ICFG-PEDES \cite{Zhu}} This dataset includes 54,552 images of 4,102 identities, each paired with one human-annotated description.
The official split provides 34,674 image–text pairs from 3,102 identities for training and 19,848 pairs from the remaining 1,000 identities for testing.

\textbf{RSTPReid \cite{dingSemanticallySelfAlignedNetwork2021}} This dataset covers 4,104 identities captured by 15 cameras, totaling 20,505 images.
Each identity has five images from different viewpoints, and each image has two descriptions.
Following the official split, training uses data from 3,701 identities, while validation and testing each use data from 200 identities.

\textbf{Evaluation Metrics} In line with existing studies, we adopt Rank-k (for k=1, 5, 10) and mean Average Precision (mAP) as the evaluation metrics for the aforementioned benchmarks.
\subsection{Implementation Details}\label{sec:4.2}
\noindent We use a pre-trained bert-base-uncased as the text encoder (hidden size 768) and vit-base-patch16-224 as the visual encoder, resizing all images to 224×224.
Image features are processed by BDAM with multi-layer self- and cross-attention, splitting them into identity and clothing features (both 768-d).
A two-layer Mamba fusion module (input/output 256, state 16, conv kernel 4) then fuses the multimodal features. Dropout is 0.1.
Training uses Adam with a learning rate of $1 \times 10^{-4}$, weight decay $1 \times 10^{-3}$, and a cosine annealing scheduler.
The multi-task objective combines InfoNCE and triplet losses with clothing adversarial, disentanglement, and gate-regularization terms.
We adopt GradNorm for adaptive task weighting with coefficient $\alpha$ of 1.5 and a weight learning rate of 0.025;
weights are normalized so $\sum w_i = T$ (where T is the number of tasks) and clipped to [$10^{-4}$, 10].
Gradient norms are computed on the last shared layer. An additional log-weight regularizer $1 \times 10^{-3}$ is applied.
For data construction, we extract style features with clip-vit-base-patch32, cluster styles via DBSCAN to form style prompts, and use ChatGPT-4o to generate style-consistent identity and clothing descriptions.
Splits are enforced at the identity level with no overlap. All experiments are run with seeds 0, 1, and 2;
we select the best checkpoint on the validation set for testing and report the mean over three runs.

\subsection{Ablation Study}
\noindent In this paper, we conduct a series of ablation studies on various components of our model: the disentanglement module, the fusion module, the loss functions, and the style feature prompts—to ensure the reproducibility and comparability of our experimental results.
\begin{table}[htbp]
    \centering
    \caption{End to end model enhancement.}
    \label{tab:enhancement_fit}
    \vspace{-0.3cm}
    \setlength{\tabcolsep}{4pt}
    \footnotesize
    \begin{tabular}{lcccc}
        \toprule
        Method          & mAP(\%) $\uparrow$ & R1(\%) $\uparrow$ & R5(\%) $\uparrow$ & R10(\%) $\uparrow$ \\
        \midrule
        Baseline        & 59.81              & 70.54             & 85.49             & 91.26              \\
        + BDAM          & 66.74              & 76.27             & 89.30             & 94.02              \\
        + Fusion        & 69.58              & 78.42             & 90.74             & 95.11              \\
        + Multi Loss    & 71.22              & 79.18             & 91.51             & 95.79              \\
        + Style Prompts & 72.61              & 79.93             & 92.95             & 96.47              \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table}
\noindent \textbf{Model Architectures.} The comparisons are shown in \cref{tab:enhancement_fit}. In the architectural ablation, progressively adding BDAM, the Mamba SSM fusion, and the multi-task loss to the base model yields consistent gains across all metrics. With the further addition of Style Prompts, the model achieves its peak performance: mAP improves from 71.22\% to \textbf{72.61\%} and Rank-1 improves from 79.18\% to \textbf{79.93\%}. This confirms that the richer textual diversity provided by the prompts enhances both the overall ranking (mAP and Rank-10) and the top-1 match accuracy. Overall, each component contributes positively to performance.
\begin{table}[htbp]
    \centering
    \caption{Ablation on the BDAM module.}
    \label{tab:ablation_bdam}
    \vspace{-0.3cm}
    \setlength{\tabcolsep}{4pt}
    \footnotesize
    \begin{tabular}{lcccc}
        \toprule
        Method              & mAP(\%) $\uparrow$ & R1(\%) $\uparrow$ & R5(\%) $\uparrow$ & R10(\%) $\uparrow$ \\
        \midrule
        Baseline(w/o BDAM)  & 59.81              & 70.54             & 85.49             & 91.26              \\
        + BDAM              & 66.74              & 76.27             & 89.30             & 94.02              \\
        w/o Cross-Attention & 62.56              & 71.39             & 87.05             & 92.98              \\
        w/o Gate            & 65.11              & 74.63             & 88.77             & 93.56              \\
        Shallow(3-layer)    & 64.27              & 73.74             & 88.09             & 93.32              \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table}

As shown in \cref{tab:ablation_bdam}, BDAM yields a substantial gain over the baseline (Rank-1: 70.54\% → 76.27\%).
Within BDAM, both bidirectional cross-attention and gating are crucial: removing cross-attention (w/o Cross-Attention) drops Rank-1 to 71.39\% (-4.88 pp), and removing the gate (w/o Gate) also degrades performance.
Using a shallower architecture further hurts results, indicating that deeper semantic modeling strengthens identity/clothing disentanglement and overall robustness.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{images/cuhk_pedes.png}
    \caption{t-SNE visualization comparison.
        (a) Baseline (w/o disentanglement) produces a highly entangled 2D point cloud with heavy identity overlap, indicating identity–clothing coupling.
        (b) With BDAM + HSIC, embeddings form tighter same-identity clusters, clearer inter-identity boundaries, and a center-to-periphery color gradient reflecting progressive identity–clothing disentanglement.}
    \label{fig:demo}
\end{figure}

To assess BDAM's disentanglement geometrically, we visualize features with t-SNE;
see \cref{fig:demo}. (a) The baseline produces a highly mixed 2D embedding with heavy identity overlap, indicating severe coupling between identity and non-identity semantics.
(b) With BDAM plus HSIC, the space becomes well structured: identity features (colored and gray dots) and clothing features (red "x") split into two independent regions.
Within the identity region, same-identity points form compact clusters with clear margins between identities.
This geometry corroborates the orthogonality induced by the disentanglement mechanism and explains the marked performance gains, consistent with \cref{tab:ablation_bdam}.

The comparisons are shown in \cref{tab:ablation_fusion}.In the Fusion module ablation, the full design (Full Fusion)—i.e., the "+Fusion" row in Table 1 built on BDAM—achieves the best performance with Rank-1 = 78.42\%.Removing any component (Mamba, gated fusion, or modal alignment) degrades results;
dropping Mamba has the largest impact (Rank-1 → 75.73\%), highlighting its role in modeling long-sequence cross-modal interactions.
Removing gating or alignment causes smaller declines, suggesting they provide auxiliary flexibility in fusion.
\begin{table}[htbp]
    \centering
    \caption{Ablation on Fusion Module.}
    \label{tab:ablation_fusion}
    \vspace{-0.3cm}
    \setlength{\tabcolsep}{4pt}
    \footnotesize
    \begin{tabular}{lcccc}
        \toprule
        Method               & mAP(\%) $\uparrow$ & R1(\%) $\uparrow$ & R5(\%) $\uparrow$ & R10(\%) $\uparrow$ \\
        \midrule
        Baseline(w/o Fusion) & 59.81              & 70.54             & 85.49             & 91.26              \\
        Full Fusion          & 69.58              & 78.42             & 90.74             & 95.11              \\
        w/o Mamba            & 66.89              & 75.73             & 89.06             & 93.92              \\
        w/o Gate-Fusion      & 68.64              & 77.58             & 90.11             & 94.87              \\
        w/o Align            & 68.15              & 77.09             & 89.84             & 94.53              \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table}
\begin{table}[htbp]
    \centering
    \caption{Ablation on Individual Loss Components.}
    \label{tab:ablation_loss}
    \vspace{-0.3cm}
    \setlength{\tabcolsep}{4pt}
    \footnotesize
    \begin{tabular}{lcccc}
        \toprule
        Method            & mAP(\%) $\uparrow$ & R1(\%) $\uparrow$ & R5(\%) $\uparrow$ & R10(\%) $\uparrow$ \\
        \midrule
        \rowcolor{gray!20}
        Full Model        & 72.61              & 79.93             & 92.95             & 96.47              \\
        w/o InfoNCE Loss  & 28.14              & 36.55             & 55.21             & 65.83              \\
        w/o Triplet Loss  & 67.22              & 74.89             & 88.15             & 93.12              \\
        w/o Adv Loss      & 69.15              & 76.92             & 89.53             & 94.22              \\
        w/o Decouple Loss & 70.03              & 77.81             & 90.11             & 94.98              \\
        w/o Gate Loss     & 71.98              & 79.23             & 91.35             & 95.71              \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table}
\noindent \textbf{Loss Functions.} To validate the necessity of each component within our designed multi-task loss framework, this section takes the best-performing dynamic weight model (Full Model) as the baseline and conducts an ablation study by removing each individual loss term one by one. The results are presented in \cref{tab:ablation_loss}.
The results show that every loss term contributes positively. InfoNCE and Triplet are pivotal: removing InfoNCE nearly collapses performance (\textbf{mAP-44.47\%}, \textbf{Rank-1-43.38\%}), underscoring cross-modal contrastive learning as foundational; dropping Triplet also causes a marked decline, confirming the need for strong intra-modal identity discrimination. Disentanglement losses (Adv, Decouple) are likewise critical—removing either degrades results, indicating architecture-only implicit disentanglement is insufficient and explicit constraints are required to separate identity from clothing. Removing the Gate regularization (w/o Gate Loss) also hurts performance, validating its auxiliary role in stabilizing the gating mechanism.
\begin{table}[htbp]
    \centering
    \caption{Ablation on style prompts.}
    \label{tab:ablation_style_prompts}
    \vspace{-0.3cm}
    \setlength{\tabcolsep}{4pt}
    \footnotesize
    \begin{tabular}{lcccc}
        \toprule
        Method               & mAP (\%) & R1 (\%) & R5 (\%) & R10 (\%) \\
        \midrule
        Baseline (w/o Style) & 71.22    & 79.18   & 91.51   & 95.79    \\
        + Random             & 71.95    & 79.54   & 91.66   & 95.82    \\
        + KMeans             & 71.85    & 79.20   & 91.60   & 95.90    \\
        + GMM                & 71.70    & 79.05   & 91.50   & 95.75    \\
        + Agglomerative      & 71.78    & 79.10   & 91.55   & 95.80    \\
        + HDBSCAN            & 72.20    & 79.40   & 91.95   & 96.10    \\
        \rowcolor{gray!20}
        + DBSCAN             & 72.61    & 79.93   & 92.95   & 96.47    \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table}

\noindent \textbf{Style Prompts.} To evaluate the effectiveness of different clustering strategies in generating style prompts, we conduct a comprehensive comparison of various clustering methods.
\cref{tab:ablation_style_prompts} reports the impact of clustering strategies for style prompts.
Relative to the multi-task baseline in Table 1, randomly sampled prompts yield only marginal gains, underscoring the importance of prompt quality.
Centroid-based methods (K-Means, GMM) provide modest improvements but require a preset number of clusters and are sensitive to varying densities, producing prompts with weaker semantic consistency;
agglomerative (hierarchical) clustering shows similar limitations. In contrast, density-based methods better handle noise and irregular cluster shapes: HDBSCAN improves robustness, while DBSCAN achieves the best mAP and Rank-10.
By avoiding a preset cluster count and automatically identifying noise, DBSCAN generates style-consistent, highly discriminative prompts, striking an optimal balance between performance and simplicity;
hence, we adopt it as our final approach.

\subsection{Efficiency Analysis}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\columnwidth]{Figure4.pdf}
    \caption{Comparative analysis of model efficiency and performance on CUHK-PEDES.
        Each subplot presents a key metric, enabling direct trade-off comparisons.
        Model abbreviations: Simple (S), Transformer-2Layer (T(2)), Performer (P), Transformer-4Layer (T(4)), and Ours.}
    \label{fig:efficiency_comparison}
\end{figure}
\noindent This section quantitatively evaluates the computational efficiency of the proposed Mamba-based fusion module.
We report key efficiency metrics and compare it against alternative fusion strategies under an identical hardware setup.

As visualized in Figure~\ref{fig:efficiency_comparison}, our proposed method demonstrates a superior balance between accuracy and efficiency. The trajectory for its mAP resides at the top of the plot, achieving a competitive \textbf{72.61\%}, which is nearly identical to the best-performing but computationally intensive \textbf{Trans(4)} model (72.99\%). Concurrently, the trend lines for all of our model's cost-related metrics—Params, FLOPs, and Training VRAM are positioned at the bottom of the chart, closely mirroring the lightweight \textbf{Simple} baseline. This combination of a high-performance trajectory with low-cost trajectories visually confirms an exceptional efficiency-to-performance ratio.

\subsection{Comparisons with State-of-the-Art Methods}
\begin{table*}[tp]
    \centering
    \caption{Comparisons with state-of-the-art ReID methods under the traditional evaluation setting.}
    \label{tab:sota_comparison}
    \vspace{-0.3cm}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l|c|cccc|cccc|cccc}
            \thickhline
            \multirow{2}{*}{\textbf{Method}}
                                                              & \multirow{2}{*}{\textbf{Backbone}}
                                                              & \multicolumn{4}{c}{\textbf{CUHK-PEDES}}
                                                              & \multicolumn{4}{c}{\textbf{ICFG-PEDES}}
                                                              & \multicolumn{4}{c}{\text{RSTPReid}}                                                                                                                                                                                  \\
            \hhline{~~|----|----|----}                        &
                                                              & \textbf{R-1}                            & \textbf{R-5}   & \textbf{R-10}  & \textbf{mAP}   & \textbf{R-1} & \textbf{R-5} & \textbf{R-10} & \textbf{mAP} & \textbf{R-1} & \textbf{R-5} & \textbf{R-10} & \textbf{mAP} \\
            \hline
            \multicolumn{14}{l}{\small\textbf{\textit{Methods with CLIP backbone:}}}                                                                                                                                                                                                 \\
            \hline
            IRRA\cite{jiangCrossModalImplicitRelation2023}    & CLIP-ViT
                                                              & 73.38                                   & 89.93          & 93.71          & 66.10          & 63.36        & 80.82        & 85.82         & 38.06        & 60.20        & 81.30        & 88.20         & 47.17        \\
            IRLT\cite{Liub}                                   & CLIP-ViT
                                                              & 73.67                                   & 89.71          & 93.57          & 65.94          & 63.57        & 80.57        & 86.32         & 38.34        & 60.51        & 82.85        & 89.71         & 47.64        \\
            CFAM\cite{Zuoa}                                   & CLIP-ViT
                                                              & 74.46                                   & 90.19          & 94.01          & -              & 64.72        & 81.35        & 86.31         & -            & 61.49        & 82.26        & 89.23         & -            \\
            Propot\cite{Yan}                                  & CLIP-ViT
                                                              & 74.89                                   & 89.90          & 94.17          & 67.12          & 65.12        & 81.57        & 86.97         & 42.93        & 61.87        & 83.63        & 89.70         & 47.82        \\
            RDE\cite{Qin}                                     & CLIP-ViT
                                                              & 75.94                                   & 90.14          & 94.12          & 67.56          & 67.68        & 82.47        & 87.36         & 40.06        & 65.35        & 83.95        & 89.90         & 50.88        \\
            HAM\cite{jiangModelingThousandsHuman2025}         & CLIP-ViT
                                                              & 77.99                                   & 91.34          & 95.03          & 69.72          & 69.95        & 83.88        & 88.39         & 42.72        & 72.50        & 87.70        & 91.95         & 55.47        \\
            \hline
            \multicolumn{14}{l}{\small\textbf{\textit{Methods with ViT backbone:}}}                                                                                                                                                                                                  \\
            \hline
            CPCL\cite{Zheng}                                  & ViT
                                                              & 70.03                                   & 87.28          & 91.78          & 63.19          & 62.60        & 79.07        & 84.46         & 36.16        & 58.35        & 81.05        & 87.65         & 45.81        \\
            PDReid\cite{liPromptDecouplingTexttoImage2024}    & ViT
                                                              & 71.59                                   & 87.95          & 92.45          & 65.03          & 60.93        & 77.96        & 84.11         & 36.44        & 56.65        & 77.40        & 84.70         & 45.27        \\
            SSAN\cite{dingSemanticallySelfAlignedNetwork2021} & ViT
                                                              & 61.37                                   & 80.15          & 86.73          & -              & 54.23        & 72.63        & 79.53         & -            & 43.50        & 67.80        & 77.15         & -            \\
            CFine\cite{Yana}                                  & ViT
                                                              & 69.57                                   & 85.93          & 91.15          & -              & 60.83        & 76.55        & 82.42         & -            & 50.55        & 72.50        & 81.60         & -            \\
            IVT\cite{Shu}                                     & ViT
                                                              & 65.59                                   & 83.11          & 89.21          & -              & 56.04        & 73.60        & 80.22         & -            & 46.70        & 70.00        & 78.80         & -            \\
            \thickhline
            \rowcolor{gray!20}
            Ours                                              & ViT
                                                              & \textbf{79.93}                          & \textbf{92.95} & \textbf{96.47} & \textbf{72.61}
                                                              & \textbf{68.68}                          & \textbf{84.29} & \textbf{89.74} & \textbf{41.78}
                                                              & \textbf{74.33}                          & \textbf{88.85} & \textbf{92.95} & \textbf{57.68}                                                                                                                           \\
            \thickhline
        \end{tabular}
    }
    \vspace{-0.2cm}
\end{table*}
\noindent To demonstrate the superior performance of our method, we conduct comprehensive comparisons with state-of-the-art text-to-image person re-identification approaches across three mainstream benchmark datasets;
results are reported in \cref{tab:sota_comparison}.

On CUHK-PEDES, our method achieves a Rank-1 accuracy of 79.93\% and an mAP of 72.61\%, marking a substantial improvement over traditional ViT-based methods (e.g., CPCL, PDReid) and outperforming the recent CLIP-based method HAM (77.99\% / 69.72\%).
This indicates that, even without leveraging CLIP's large-scale pre-trained weights, our identity–clothing decoupling structure and efficient fusion strategy deliver superior cross-modal alignment.
On ICFG-PEDES, our approach attains a Rank-1 accuracy of 68.68\%, comparable to HAM (69.95\%) and significantly better than methods such as RDE and Propot, further validating the effectiveness of decoupled modeling under clothing variations.
On RSTPReid, we achieve the current best results—Rank-1 of 74.33\% and mAP of 57.68\%—surpassing all ViT-based methods and substantially outperforming HAM (72.50\% / 55.47\%).
These gains underscore the robustness and generalization of our decoupling and fusion mechanism in complex, multi-camera, cross-scene settings.
In summary, our method performs strongly across all three public datasets, setting new state-of-the-art results on CUHK-PEDES and RSTPReid while remaining highly competitive on ICFG-PEDES.
These findings consistently validate the effectiveness of the Bidirectional Decoupling Alignment Module for Identity and Clothing (BDAM), the Mamba-based fusion strategy, and the importance of multi-task optimization with dynamic weight adjustment for cross-modal person re-identification.
\section{Conclusion and Limitations}
\label{sec:conclusion}
\noindent In this work, we proposed a novel framework to address clothing-induced interference in text-to-image person re-identification, centered on MLLM-supervised feature decoupling. We successfully demonstrated that a Multimodal Large Language Model can provide fine-grained supervision to guide our Bidirectional Decoupling Alignment Module. By design, this module explicitly isolates identity-relevant features while actively suppressing clothing-related interference through a combined adversarial and kernel-based orthogonal loss strategy. Furthermore, our integration of a Mamba State Space Model proved to be an effective and efficient fusion strategy, adept at capturing cross-modal dependencies. Our method's effectiveness was validated by achieving new state-of-the-art performance on the CUHK-PEDES and RSTPReid benchmarks

Nonetheless, limitations remain, primarily concerning the potential for noise in MLLM-generated descriptions and the need for further optimization for large-scale deployment. Future work will focus on improving description reliability to mitigate noise and enhancing inference efficiency. We also plan to explore the framework's applicability to more challenging scenarios, such as clothing-changing person Re-ID, to further test the robustness of our decoupling mechanism.
    % --- 参考文献 ---
    {\small
        \bibliographystyle{ieee_fullname}
        \bibliography{egbib}
    }
\end{document}