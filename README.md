# Robust Person Re-Identification via MLLM-Supervised Feature Decoupling
[![LaTeX](https://img.shields.io/badge/LaTeX-008080?style=for-the-badge&logo=latex&logoColor=white)](https://www.latex-project.org/)

***This repository contains the official LaTeX source code for our paper:***

***Aurhor:*** *<u>KeLong Yan</u>*

***arXiv:*** *<u>To be determined</u>*

***Gthub Code:***[yyykkklll/Text-based-Person-Retrieval_V3: ðŸŽ“åŸºäºŽæ–‡æœ¬æŒ‡å¯¼çš„è¡Œäººé‡è¯†åˆ«ï¼ˆText-to-Image ReIDï¼‰æ¨¡åž‹ï¼Œé‡‡ç”¨ ViT + BERT æž¶æž„ï¼Œå¹¶èžåˆé—¨æŽ§æœºåˆ¶ä¸Žèº«ä»½-è¡£ç‰©è§£è€¦è®¾è®¡ ðŸš€ (github.com)](https://github.com/yyykkklll/Text-based-Person-Retrieval_V3)

---

## Abstract

> ***The challenge of fine-grained semantic alignment contin ues to hinder Text-to-Image Person Re-Identification, where clothing-induced interference and a persistent modality gap degrade retrieval accuracy. We introduce a novel framework that decouples identity and clothing features through Multi-Modal Large Language Model (MLLM)-guided supervision, enabling robust cross-modal matching.***

## Framework

***Our core approach involves decoupling traditionally coupled features (Figure a) into separate components (Figure b). We utilize the refined text generated by MLLM (identity/clothing) as supervision to force the model to decouple identity and clothing features, achieving robust cross-modal retrieval.***

### **Detailed Architecture**

***The figure below illustrates the detailed workflow of our proposed method, encompassing the BDAM module, the MLLM text generation process, and the multi-task loss employed for forced decoupling.***

![](images/frame.png)

## Warehouse Contents

***This repository contains all the LaTeX source files required to compile our paper, including:***

* ***`main.tex`: Main .tex file.***
* ***`*.bib`: References document.***
* ***`figures/`: Includes all images and charts used in the paper.***
