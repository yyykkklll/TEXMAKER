@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@misc{wangDisentangledRepresentationLearning2024,
  title        = {Disentangled {{Representation Learning}}},
  author       = {Wang, Xin and Chen, Hong and Tang, Si'ao and Wu, Zihao and Zhu, Wenwu},
  year         = {2024},
  eprint       = {2211.11695},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2211.11695}
}

@inproceedings{liuMultitaskAdversarialNetwork2018,
  title     = {Multi-Task {{Adversarial Network}} for {{Disentangled Feature Learning}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author    = {Liu, Yang and Wang, Zhaowen and Jin, Hailin and Wassell, Ian},
  year      = {2018},
  pages     = {3743--3751},
  address   = {Salt Lake City, UT, USA},
  doi       = {10.1109/CVPR.2018.00394},
  isbn      = {978-1-5386-6420-9}
}

@misc{sanchezLearningDisentangledRepresentations2019,
  title        = {Learning {{Disentangled Representations}} via {{Mutual Information Estimation}}},
  author       = {Sanchez, Eduardo Hugo and Serrurier, Mathieu and Ortner, Mathias},
  year         = {2019},
  eprint       = {1912.03915},
  primaryclass = {stat},
  doi          = {10.48550/arXiv.1912.03915}
}

@inproceedings{chengImprovingDisentangledText2020,
  title     = {Improving {{Disentangled Text Representation Learning}} with {{Information-Theoretic Guidance}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author    = {Cheng, Pengyu and Min, Martin Renqiang and Shen, Dinghan and Malon, Christopher and Zhang, Yizhe and Li, Yitong and Carin, Lawrence},
  year      = {2020},
  pages     = {7530--7541},
  address   = {Online},
  doi       = {10.18653/v1/2020.acl-main.673}
}

@misc{materzynskaDisentanglingVisualWritten2022,
  title        = {Disentangling Visual and Written Concepts in {{CLIP}}},
  author       = {Materzynska, Joanna and Torralba, Antonio and Bau, David},
  year         = {2022},
  eprint       = {2206.07835},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2206.07835}
}

@misc{chengDisentangledFeatureRepresentation2021,
  title        = {Disentangled {{Feature Representation}} for {{Few-shot Image Classification}}},
  author       = {Cheng, Hao and Wang, Yufei and Li, Haoliang and Kot, Alex C. and Wen, Bihan},
  year         = {2021},
  eprint       = {2109.12548},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2109.12548}
}

@misc{ganinUnsupervisedDomainAdaptation2015,
  title        = {Unsupervised {{Domain Adaptation}} by {{Backpropagation}}},
  author       = {Ganin, Yaroslav and Lempitsky, Victor},
  year         = {2015},
  eprint       = {1409.7495},
  primaryclass = {stat},
  doi          = {10.48550/arXiv.1409.7495}
}

@inproceedings{liDisentanglingIdentityFeatures2024,
  title     = {Disentangling {{Identity Features}} from {{Interference Factors}} for {{Cloth-Changing Person Re-identification}}},
  booktitle = {Proceedings of the 32nd {{ACM International Conference}} on {{Multimedia}}},
  author    = {Li, Yubo and Cheng, De and Fang, Chaowei and Jiao, Changzhe and Wang, Nannan and Gao, Xinbo},
  year      = {2024},
  pages     = {2252--2261},
  address   = {Melbourne VIC Australia},
  doi       = {10.1145/3664647.3680823},
  isbn      = {979-8-4007-0686-8}
}

@misc{azadActivityBiometricsPersonIdentification2024,
  title        = {Activity-{{Biometrics}}: {{Person Identification}} from {{Daily Activities}}},
  author       = {Azad, Shehreen and Rawat, Yogesh Singh},
  year         = {2024},
  eprint       = {2403.17360},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2403.17360}
}

@inproceedings{baiDisentangledFeatureLearning2020,
  title     = {Disentangled {{Feature Learning Network}} for {{Vehicle Re-Identification}}},
  booktitle = {Proceedings of the {{Twenty-Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  author    = {Bai, Yan and Lou, Yihang and Dai, Yongxing and Liu, Jun and Chen, Ziqian and Duan, Ling-Yu},
  year      = {2020},
  pages     = {474--480},
  address   = {Yokohama, Japan},
  doi       = {10.24963/ijcai.2020/66},
  isbn      = {978-0-9992411-6-5}
}

@misc{cuiProFDPromptGuidedFeature2024,
  title        = {{{ProFD}}: {{Prompt-Guided Feature Disentangling}} for {{Occluded Person Re-Identification}}},
  author       = {Cui, Can and Huang, Siteng and Song, Wenxuan and Ding, Pengxiang and Zhang, Min and Wang, Donglin},
  year         = {2024},
  eprint       = {2409.20081},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2409.20081},
  annotation   = {TLDR: A Prompt-guided Feature Disentangling method (ProFD), which leverages the rich pre-trained knowledge in the textual modality facilitate model to generate well-aligned part features and adopts a hybrid-attention decoder, ensuring spatial and semantic consistency during the decoding process to minimize noise impact.}
}

@misc{yinGraFTGradualFusion2023,
  title        = {{{GraFT}}: {{Gradual Fusion Transformer}} for {{Multimodal Re-Identification}}},
  author       = {Yin, Haoli and Li, Jiayao and Schiller, Eva and McDermott, Luke and Cummings, Daniel},
  year         = {2023},
  eprint       = {2310.16856},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2310.16856},
  annotation   = {TLDR: This work introduces the GraFT, a novel training paradigm combined with an augmented triplet loss, optimizing the ReID feature embedding space and demonstrating that GraFT consistently surpasses established multimodal ReID benchmarks.}
}

@inproceedings{yuan2021transformer,
  title     = {Transformer-based Feature Reconstruction Network for Robust Multimodal Sentiment Analysis},
  author    = {Yuan, Ziqi and Li, Wei and Xu, Hua and Yu, Wenmeng},
  booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
  pages     = {4400--4407},
  year      = {2021}
}

@misc{kimExtendingCLIPImageText2024,
  title        = {Extending {{CLIP}}'s {{Image-Text Alignment}} to {{Referring Image Segmentation}}},
  author       = {Kim, Seoyeon and Kang, Minguk and Kim, Dongwon and Park, Jaesik and Kwak, Suha},
  year         = {2024},
  eprint       = {2306.08498},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2306.08498}
}

@misc{liLearningGraphNeural2023,
  title        = {Learning a {{Graph Neural Network}} with {{Cross Modality Interaction}} for {{Image Fusion}}},
  author       = {Li, Jiawei and Chen, Jiansheng and Liu, Jinyuan and Ma, Huimin},
  year         = {2023},
  eprint       = {2308.03256},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2308.03256}
}

@article{hanafiFastDBSCANAlgorithm2022,
  title      = {A Fast {{DBSCAN}} Algorithm for Big Data Based on Efficient Density Calculation},
  author     = {Hanafi, Nooshin and Saadatfar, Hamid},
  year       = {2022},
  journal    = {Expert Systems with Applications},
  volume     = {203},
  pages      = {117501},
  doi        = {10.1016/j.eswa.2022.117501},
  annotation = {Read\_Status: New\\
                Read\_Status\_Date: 2025-09-02T07:49:24.487Z}
}

@misc{liPersonSearchNaturalLanguageDescription2017,
  title        = {Person {{Search}} with {{Natural Language Description}}},
  author       = {Li, Shuang and Xiao, Tong and Li, Hongsheng and Zhou, Bolei and Yue, Dayu and Wang, Xiaogang},
  year         = {2017},
  eprint       = {1702.05729},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.1702.05729}
}

@misc{zhuDSSLDeepSurroundings2021,
  title        = {{{DSSL}}: {{Deep Surroundings-person Separation Learning}} for {{Text-based Person Retrieval}}},
  author       = {Zhu, Aichun and Wang, Zijie and Li, Yifeng and Wan, Xili and Jin, Jing and Wang, Tian and Hu, Fangqiang and Hua, Gang},
  year         = {2021},
  eprint       = {2109.05534},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2109.05534}
}

@misc{liuMoAVRMixtureofAgentsSystem2025,
  title        = {{{MoA-VR}}: {{A Mixture-of-Agents System Towards All-in-One Video Restoration}}},
  author       = {Liu, Lu and Cai, Chunlei and Shen, Shaocheng and Liang, Jianfeng and Ouyang, Weimin and Ye, Tianxiao and Mao, Jian and Duan, Huiyu and Yao, Jiangchao and Zhang, Xiaoyun and Hu, Qiang and Zhai, Guangtao},
  year         = {2025},
  eprint       = {2510.08508},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2510.08508}
}

@misc{schmidtRobustCanonicalizationBootstrapped2025,
  title        = {Robust {{Canonicalization}} through {{Bootstrapped Data Re-Alignment}}},
  author       = {Schmidt, Johann and Stober, Sebastian},
  year         = {2025},
  eprint       = {2510.08178},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2510.08178}
}

@misc{zuoUFineBenchTowardsTextbased2024,
  title        = {{{UFineBench}}: {{Towards Text-based Person Retrieval}} with {{Ultra-fine Granularity}}},
  author       = {Zuo, Jialong and Zhou, Hanyu and Nie, Ying and Zhang, Feng and Guo, Tianyu and Sang, Nong and Wang, Yunhe and Gao, Changxin},
  year         = {2024},
  eprint       = {2312.03441},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2312.03441}
}

@article{liuCausalityInspiredInvariantRepresentation2024,
  title      = {Causality-{{Inspired Invariant Representation Learning}} for {{Text-Based Person Retrieval}}},
  author     = {Liu, Yu and Qin, Guihe and Chen, Haipeng and Cheng, Zhiyong and Yang, Xun},
  year       = {2024},
  journal    = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume     = {38},
  pages      = {14052--14060},
  doi        = {10.1609/aaai.v38i12.29314},
  annotation = {TLDR: This work pioneer the observation of TPR from a causal view, and proposes an Invariant Representation Learning method for TPR (IRLT), that enforces the visual representations to satisfy the two critical properties of causal/non-causal factors.}
}

@misc{yanPrototypicalPromptingTexttoimage2024,
  title        = {Prototypical {{Prompting}} for {{Text-to-image Person Re-identification}}},
  author       = {Yan, Shuanglin and Liu, Jun and Dong, Neng and Zhang, Liyan and Tang, Jinhui},
  year         = {2024},
  eprint       = {2409.09427},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2409.09427}
}

@misc{qinNoisyCorrespondenceLearningTexttoImage2024,
  title        = {Noisy-{{Correspondence Learning}} for {{Text-to-Image Person Re-identification}}},
  author       = {Qin, Yang and Chen, Yingke and Peng, Dezhong and Peng, Xi and Zhou, Joey Tianyi and Hu, Peng},
  year         = {2024},
  eprint       = {2308.09911},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2308.09911}
}

@misc{zhengCPCLCrossModalPrototypical2024,
  title        = {{{CPCL}}: {{Cross-Modal Prototypical Contrastive Learning}} for {{Weakly Supervised Text-based Person Re-Identification}}},
  author       = {Zheng, Yanwei and Zhao, Xinpeng and Lan, Chuanlin and Zhang, Xiaowei and Huang, Bowen and Yang, Jibin and Yu, Dongxiao},
  year         = {2024},
  eprint       = {2401.10011},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2401.10011},
  annotation   = {TLDR: The proposed Cross-Modal Prototypical Contrastive Learning (CPCL) method introduces the CLIP model to weakly supervised TPRe-ID for the first time, mapping visual and textual instances into a shared latent space and captures associations between heterogeneous modalities of image-text pairs belonging to the same person.}
}

@misc{charpentierStrongerReidentificationAttacks2025,
  title        = {Stronger {{Re-identification Attacks}} through {{Reasoning}} and {{Aggregation}}},
  author       = {Charpentier, Lucas Georges Gabriel and Lison, Pierre},
  year         = {2025},
  eprint       = {2510.09184},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2510.09184}
}

@misc{fengKnowledgeGuidedDynamicModality2024,
  title        = {Knowledge-{{Guided Dynamic Modality Attention Fusion Framework}} for {{Multimodal Sentiment Analysis}}},
  author       = {Feng, Xinyu and Lin, Yuming and He, Lihua and Li, You and Chang, Liang and Zhou, Ya},
  year         = {2024},
  eprint       = {2410.04491},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2410.04491},
  annotation   = {TLDR: A Knowledge-Guided Dynamic Modality Attention Fusion Framework (KuDA) for multimodal sentiment analysis that achieves state-of-the-art performance and is able to adapt to different scenarios of dominant modality.}
}

@misc{yanCLIPDrivenFinegrainedTextImage2022,
  title        = {{{CLIP-Driven Fine-grained Text-Image Person Re-identification}}},
  author       = {Yan, Shuanglin and Dong, Neng and Zhang, Liyan and Tang, Jinhui},
  year         = {2022},
  eprint       = {2210.10276},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2210.10276}
}

@inproceedings{tranDisentangledRepresentationLearning2017,
  title     = {Disentangled {{Representation Learning GAN}} for {{Pose-Invariant Face Recognition}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author    = {Tran, Luan and Yin, Xi and Liu, Xiaoming},
  year      = {2017},
  pages     = {1283--1292},
  address   = {Honolulu, HI},
  doi       = {10.1109/CVPR.2017.141},
  isbn      = {978-1-5386-0457-1}
}

@misc{gaoContextualNonLocalAlignment2021,
  title        = {Contextual {{Non-Local Alignment}} over {{Full-Scale Representation}} for {{Text-Based Person Search}}},
  author       = {Gao, Chenyang and Cai, Guanyu and Jiang, Xinyang and Zheng, Feng and Zhang, Jun and Gong, Yifei and Peng, Pai and Guo, Xiaowei and Sun, Xing},
  year         = {2021},
  eprint       = {2101.03036},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2101.03036}
}

@article{bukhariLanguageVisionBased2023,
  title   = {Language and Vision Based Person Re-Identification for Surveillance Systems Using Deep Learning with {{LIP}} Layers},
  author  = {Bukhari, Maryam and Yasmin, Sadaf and Naz, Sheneela and Maqsood, Muazzam and Rew, Jehyeok and Rho, Seungmin},
  year    = {2023},
  journal = {Image and Vision Computing},
  volume  = {132},
  pages   = {104658},
  doi     = {10.1016/j.imavis.2023.104658}
}

@misc{dingSemanticallySelfAlignedNetwork2021,
  title        = {Semantically {{Self-Aligned Network}} for {{Text-to-Image Part-aware Person Re-identification}}},
  author       = {Ding, Zefeng and Ding, Changxing and Shao, Zhiyin and Tao, Dacheng},
  year         = {2021},
  eprint       = {2107.12666},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2107.12666}
}

@misc{dosovitskiyImageWorth16x162021,
  title        = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  author       = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year         = {2021},
  eprint       = {2010.11929},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2010.11929}
}

@misc{farooqAXMNetImplicitCrossModal2022,
  title        = {{{AXM-Net}}: {{Implicit Cross-Modal Feature Alignment}} for {{Person Re-identification}}},
  author       = {Farooq, Ammarah and Awais, Muhammad and Kittler, Josef and Khalid, Syed Safwan},
  year         = {2022},
  eprint       = {2101.08238},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2101.08238}
}

@misc{galiyawalaPersonRetrievalSurveillance2021,
  title        = {Person {{Retrieval}} in {{Surveillance Using Textual Query}}: {{A Review}}},
  author       = {Galiyawala, Hiren and Raval, Mehul S.},
  year         = {2021},
  eprint       = {2105.02414},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2105.02414}
}

@misc{guWukong100Million2022,
  title        = {Wukong: {{A}} 100 {{Million Large-scale Chinese Cross-modal Pre-training Benchmark}}},
  author       = {Gu, Jiaxi and Meng, Xiaojun and Lu, Guansong and Hou, Lu and Niu, Minzhe and Liang, Xiaodan and Yao, Lewei and Huang, Runhui and Zhang, Wei and Jiang, Xin and Xu, Chunjing and Xu, Hang},
  year         = {2022},
  eprint       = {2202.06767},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2202.06767}
}

@misc{heInstructReIDUniversalPurpose2025,
  title        = {Instruct-{{ReID}}++: {{Towards Universal Purpose Instruction-Guided Person Re-identification}}},
  author       = {He, Weizhen and Deng, Yiheng and Yan, Yunfeng and Zhu, Feng and Wang, Yizhou and Bai, Lei and Xie, Qingsong and Qi, Donglian and Ouyang, Wanli and Tang, Shixiang},
  year         = {2025},
  eprint       = {2405.17790},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2405.17790}
}

@misc{jiangCrossModalImplicitRelation2023,
  title        = {Cross-{{Modal Implicit Relation Reasoning}} and {{Aligning}} for {{Text-to-Image Person Retrieval}}},
  author       = {Jiang, Ding and Ye, Mang},
  year         = {2023},
  eprint       = {2303.12501},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2303.12501}
}

@misc{liBLIPBootstrappingLanguageImage2022,
  title        = {{{BLIP}}: {{Bootstrapping Language-Image Pre-training}} for {{Unified Vision-Language Understanding}} and {{Generation}}},
  author       = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  year         = {2022},
  eprint       = {2201.12086},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2201.12086}
}

@misc{liPromptDecouplingTexttoImage2024,
  title        = {Prompt {{Decoupling}} for {{Text-to-Image Person Re-identification}}},
  author       = {Li, Weihao and Tan, Lei and Dai, Pingyang and Zhang, Yan},
  year         = {2024},
  eprint       = {2401.02173},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2401.02173},
  annotation   = {TLDR: This work introduces the prompt tuning strategy to enable domain adaptation and proposes a two-stage training approach to disentangle domain adaptation from task adaptation for text-to-image person re-identification.}
}

@article{niuLLMLocBootstrapSingleimage2025,
  title   = {{{LLM-Loc}}: {{Bootstrap}} Single-Image Indoor Localization with Large Language Model},
  author  = {Niu, Qun and Chen, Tao and Zhang, Xing and Wang, Yifan and Liu, Ning},
  year    = {2025},
  journal = {Expert Systems with Applications},
  volume  = {291},
  pages   = {128368},
  doi     = {10.1016/j.eswa.2025.128368}
}

@misc{radfordLearningTransferableVisual2021,
  title        = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author       = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year         = {2021},
  eprint       = {2103.00020},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2103.00020}
}

@inproceedings{wangCAIBCCapturingAllround2022,
  title        = {{{CAIBC}}: {{Capturing All-round Information Beyond Color}} for {{Text-based Person Retrieval}}},
  booktitle    = {Proceedings of the 30th {{ACM International Conference}} on {{Multimedia}}},
  author       = {Wang, Zijie and Zhu, Aichun and Xue, Jingyi and Wan, Xili and Liu, Chao and Wang, Tian and Li, Yifeng},
  year         = {2022},
  eprint       = {2209.05773},
  primaryclass = {cs},
  pages        = {5314--5322},
  doi          = {10.1145/3503161.3548057}
}

@misc{shaoLearningGranularityUnifiedRepresentations2022,
  title        = {Learning {{Granularity-Unified Representations}} for {{Text-to-Image Person Re-identification}}},
  author       = {Shao, Zhiyin and Zhang, Xinyu and Fang, Meng and Lin, Zhifeng and Wang, Jian and Ding, Changxing},
  year         = {2022},
  eprint       = {2207.07802},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2207.07802}
}

@misc{tanHarnessingPowerMLLMs2024,
  title        = {Harnessing the {{Power}} of {{MLLMs}} for {{Transferable Text-to-Image Person ReID}}},
  author       = {Tan, Wentao and Ding, Changxing and Jiang, Jiayu and Wang, Fei and Zhan, Yibing and Tao, Dapeng},
  year         = {2024},
  eprint       = {2405.04940},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2405.04940}
}

@misc{wangViTAAVisualTextualAttributes2020,
  title        = {{{ViTAA}}: {{Visual-Textual Attributes Alignment}} in {{Person Search}} by {{Natural Language}}},
  author       = {Wang, Zhe and Fang, Zhiyuan and Wang, Jun and Yang, Yezhou},
  year         = {2020},
  eprint       = {2005.07327},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2005.07327}
}

@misc{yangUnifiedTextbasedPerson2023,
  title        = {Towards {{Unified Text-based Person Retrieval}}: {{A Large-scale Multi-Attribute}} and {{Language Search Benchmark}}},
  author       = {Yang, Shuyu and Zhou, Yinan and Wang, Yaxiong and Wu, Yujiao and Zhu, Li and Zheng, Zhedong},
  year         = {2023},
  eprint       = {2306.02898},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2306.02898}
}

@misc{yanLearningComprehensiveRepresentations2023,
  title        = {Learning {{Comprehensive Representations}} with {{Richer Self}} for {{Text-to-Image Person Re-Identification}}},
  author       = {Yan, Shuanglin and Dong, Neng and Liu, Jun and Zhang, Liyan and Tang, Jinhui},
  year         = {2023},
  eprint       = {2310.11210},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2310.11210}
}

@misc{yaoFILIPFinegrainedInteractive2021,
  title        = {{{FILIP}}: {{Fine-grained Interactive Language-Image Pre-Training}}},
  author       = {Yao, Lewei and Huang, Runhui and Hou, Lu and Lu, Guansong and Niu, Minzhe and Xu, Hang and Liang, Xiaodan and Li, Zhenguo and Jiang, Xin and Xu, Chunjing},
  year         = {2021},
  eprint       = {2111.07783},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2111.07783}
}

@misc{zhangDiverseEmbeddingExpansion2023,
  title        = {Diverse {{Embedding Expansion Network}} and {{Low-Light Cross-Modality Benchmark}} for {{Visible-Infrared Person Re-identification}}},
  author       = {Zhang, Yukang and Wang, Hanzi},
  year         = {2023},
  eprint       = {2303.14481},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2303.14481}
}

@misc{zhaoLuoJiaHOGHierarchyOriented2024,
  title        = {{{LuoJiaHOG}}: {{A Hierarchy Oriented Geo-aware Image Caption Dataset}} for {{Remote Sensing Image-Text Retrival}}},
  author       = {Zhao, Yuanxin and Zhang, Mi and Yang, Bingnan and Zhang, Zhan and Kang, Jiaju and Gong, Jianya},
  year         = {2024},
  eprint       = {2403.10887},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2403.10887},
  annotation   = {TLDR: An image caption dataset LuojiaHOG, which is geospatial-aware, label-extension-friendly and comprehensive-captioned, and a CLIP-based Image Semantic Enhancement Network (CISEN) to promote sophisticated ITR, which can serve as a foundational resource for future RS image-text alignment research.}
}

@misc{zuoPLIPLanguageImagePretraining2024,
  title        = {{{PLIP}}: {{Language-Image Pre-training}} for {{Person Representation Learning}}},
  author       = {Zuo, Jialong and Hong, Jiahao and Zhang, Feng and Yu, Changqian and Zhou, Hanyu and Gao, Changxin and Sang, Nong and Wang, Jingdong},
  year         = {2024},
  eprint       = {2305.08386},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2305.08386},
  annotation   = {TLDR: A novel language-image pre-training framework for person representation learning, termed PLIP, which significantly improves existing methods on all these tasks, but also shows great ability in the zero-shot and domain generalization settings.}
}

@misc{jiangModelingThousandsHuman2025,
  title        = {Modeling {{Thousands}} of {{Human Annotators}} for {{Generalizable Text-to-Image Person Re-identification}}},
  author       = {Jiang, Jiayu and Ding, Changxing and Tan, Wentao and Wang, Junhong and Tao, Jin and Xu, Xiangmin},
  year         = {2025},
  eprint       = {2503.09962},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2503.09962}
}

@misc{wangLearningDeepStructurePreserving2016,
  title        = {Learning {{Deep Structure-Preserving Image-Text Embeddings}}},
  author       = {Wang, Liwei and Li, Yin and Lazebnik, Svetlana},
  year         = {2016},
  eprint       = {1511.06078},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.1511.06078}
}

@inproceedings{wangLanguagePersonSearch2019,
  title     = {Language {{Person Search}} with {{Mutually Connected Classification Loss}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author    = {Wang, Yuyu and Bo, Chunjuan and Wang, Dong and Wang, Shuang and Qi, Yunwei and Lu, Huchuan},
  year      = {2019},
  pages     = {2057--2061},
  address   = {Brighton, United Kingdom},
  doi       = {10.1109/ICASSP.2019.8682456},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn      = {978-1-4799-8131-1}
}

@misc{liPersonSearchNatural2017,
  title        = {Person {{Search}} with {{Natural Language Description}}},
  author       = {Li, Shuang and Xiao, Tong and Li, Hongsheng and Zhou, Bolei and Yue, Dayu and Wang, Xiaogang},
  year         = {2017},
  eprint       = {1702.05729},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.1702.05729}
}

@misc{shaoUnifiedPretrainingPseudo2023,
  title        = {Unified {{Pre-training}} with {{Pseudo Texts}} for {{Text-To-Image Person Re-identification}}},
  author       = {Shao, Zhiyin and Zhang, Xinyu and Ding, Changxing and Wang, Jian and Wang, Jingdong},
  year         = {2023},
  eprint       = {2309.01420},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2309.01420}
}

@misc{shuSeeFinerSeeMore2022,
  title        = {See {{Finer}}, {{See More}}: {{Implicit Modality Alignment}} for {{Text-based Person Retrieval}}},
  author       = {Shu, Xiujun and Wen, Wei and Wu, Haoqian and Chen, Keyu and Song, Yiran and Qiao, Ruizhi and Ren, Bo and Wang, Xiao},
  year         = {2022},
  eprint       = {2208.08608},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2208.08608},
  annotation   = {TLDR: An Implicit Visual-Textual (IVT) framework for text-based person retrieval is introduced and two implicit semantic alignment paradigms are proposed: multi-level alignment (MLA) and bidirectional mask modeling (BMM).}
}



