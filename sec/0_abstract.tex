% --- 摘要 ---
\begin{abstract}
    \noindent Text-to-Image Person Re-Identification is critically hampered by the difficulty of fine-grained semantic alignment, as retrieval accuracy is degraded by clothing-induced interference and a persistent modality gap. 
    In this paper, we propose a novel framework to resolve this issue, centered on feature decoupling guided by a Multimodal Large Language Model (MLLM). 
    Our framework introduces two core components: a Bidirectional Decoupled Alignment Module and a Mamba State Space Model (SSM) for efficient fusion. 
    To obtain high-quality, fine-grained supervision, we first employ MLLM to automatically generate separate identity and clothing descriptions.
    These descriptions then guide our decoupling module, which utilizes bidirectional attention and a gated weighting strategy to meticulously disentangle visual features into identity and clothing subspaces. 
    To enforce this separation and ensure identity purity, we design a multi-task loss strategy comprising an alignment  loss that actively suppresses the influence of clothing-related features, and a kernel-based orthogonal constraint that ensures statistical independence. Furthermore, we pioneer the integration of the Mamba SSM into cross-modal Re-ID as an efficient fusion module. 
    By leveraging its linear-time complexity and proficiency in modeling long-range dependencies, it facilitates deep contextual interactions across modalities while avoiding the quadratic complexity of Transformers. 
    Comprehensive experiments on multiple benchmark datasets reveal that our proposed method achieves superior performance compared to leading contemporary methods, proving its effectiveness and robustness.
\end{abstract}

\noindent \textbf{Keywords:} Multimodal Learning, Text-to-Image Re-Identification, Feature Decoupling, Semantic Supervision.