% --- 摘要 ---
\begin{abstract}
    \noindent The challenge of fine-grained semantic alignment continues to hinder Text-to-Image Person Re-Identification, where clothing-induced interference and a persistent modality gap degrade retrieval accuracy. In this paper, we propose a novel framework to resolve this issue, centered on MLLM-supervised feature decoupling. Our framework introduces two core components: a Bidirectional Decoupling Alignment Module and a Mamba State Space Model for efficient fusion. To obtain high-quality, fine-grained supervision, we first employ a Multimodal Large Language Model to automatically generate separate identity and clothing descriptions. These descriptions then guide our decoupling module, which utilizes bidirectional attention and a gated weighting strategy to meticulously disentangle visual features into identity and clothing subspaces. To enforce this separation and ensure identity purity, we design a multi-task loss strategy comprising an adversarial loss that actively suppresses the influence of clothing-related features, and a kernel-based orthogonal constraint that ensures statistical independence. Furthermore, we are the first to integrate the Mamba State Space Model into cross-modal Re-ID as an efficient fusion module. By leveraging its linear-time complexity and proficiency in modeling long-range dependencies, it facilitates deep contextual interactions across modalities while avoiding the quadratic complexity of Transformers. Comprehensive experiments on multiple benchmark datasets reveal that our proposed method achieves superior performance compared to leading contemporary methods, proving its effectiveness and robustness.
\end{abstract}

\noindent \textbf{Keywords:} Text-to-Image Re-Identification, Feature Decoupling, MLLM, Mamba State Space Model.