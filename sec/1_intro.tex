% --- 正文 ---
\section{Introduction}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{Figure1}
    \caption{Comparison of ReID methods. (a) \textit{Traditional}: Fuses global image and text features, confounding identity cues with non-identity information. (b) \textit{Proposed BDAM}: Introduces a decoupling module to align separated identity and clothing features, guided by MLLM-generated descriptions for fine-grained matching.}
    \label{fig:figure1}
\end{figure}
\label{sec:intro}
\noindent Text-to-Image Person Re-Identificatio (T2I-ReID) retrieves a target pedestrian from a large-scale image gallery given a natural-language description~\cite{jiangCrossModalImplicitRelation2023, yanLearningComprehensiveRepresentations2023,shaoLearningGranularityUnifiedRepresentations2022, dingSemanticallySelfAlignedNetwork2021}.
It is valuable for video surveillance~\cite{bukhariLanguageVisionBased2023}, intelligent security~\cite{galiyawalaPersonRetrievalSurveillance2021}, public safety, and social media.
Despite recent progress, practical deployment remains challenging: image factors (pose, viewpoint, illumination) obscure identity-relevant cues, and a persistent modality gap hampers effective fusion in a shared space.
These issues are exacerbated at the fine-grained level, where semantic alignment is particularly difficult.

\noindent A core challenge in T2I-ReID is the semantic gap between images and text.
Early work attempted to reduce this discrepancy by projecting global visual and textual features into a shared space~\cite{wangLearningDeepStructurePreserving2016,wangLanguagePersonSearch2019,liPersonSearchNatural2017}, but high intra-class variance and low inter-class variance across both modalities hinder reliable cross-modal matching.
To overcome this, subsequent studies introduced feature disentanglement, broadly via explicit or implicit alignment~\cite{gaoContextualNonLocalAlignment2021,wangCAIBCCapturingAllround2022}.
Explicit methods~\cite{wangViTAAVisualTextualAttributes2020,dingSemanticallySelfAlignedNetwork2021} detect body parts or attributes and align local regions and phrases with auxiliary modules.
Implicit methods~\cite{jiangCrossModalImplicitRelation2023,shaoLearningGranularityUnifiedRepresentations2022} avoid external tools and use regularizers to associate noun phrases with image regions.
This progression demonstrates that distinguishing identity-relevant from identity-irrelevant semantics is essential;
therefore, disentanglement has become a key avenue to advance T2I-ReID.

\noindent This pursuit of disentanglement has been propelled by powerful backbones. Models employing ViT~\cite{dosovitskiyImageWorth16x162021,tanHarnessingPowerMLLMs2024,zuoPLIPLanguageImagePretraining2024,yangUnifiedTextbasedPerson2023,heInstructReIDUniversalPurpose2025,liPromptDecouplingTexttoImage2024} excel at capturing fine-grained visual details, while methods leveraging CLIP~\cite{radfordLearningTransferableVisual2021,yaoFILIPFinegrainedInteractive2021,niuLLMLocBootstrapSingleimage2025} utilize large-scale pretraining to learn a well-aligned joint embedding space. 
These architectures, often paired with cross-attention, have significantly advanced the state-of-the-art in cross-modal alignment.
Despite this progress, a critical limitation persists. Both lines of work commonly treat the textual description holistically.
This approach overlooks the semantic distinction between content relevant to identity (e.g., gender, body shape) and content irrelevant to identity (e.g., clothing, hairstyle). 
In complex scenes, this coarse-grained treatment forces the model to entangle these factors, often prioritizing salient but non-essential clothing details over stable identity cues. 
This ambiguity blurs identity features, weakens the decoupling process, and ultimately degrades matching robustness.

\noindent To address this challenge, we propose a novel framework that moves beyond such holistic feature treatment by introducing explicit, fine-grained semantic supervision.
Inspired by the style-clustering paradigm~\cite{jiangModelingThousandsHuman2025}, our approach employs MLLM to guide feature decoupling. 
This is achieved by automatically generating fine-grained, distinct, and mutually exclusive descriptions for both identity and clothing.
These decoupled annotations provide the precise supervision for our BDAM to meticulously separate and align identity and clothing information.
This disentanglement is enforced by a multi-task loss strategy, including an alignment loss and an orthogonality constraint based on HSIC, as detailed in our \cref{sec:method}.
Furthermore, to enhance alignment, we pioneer the integration of the Mamba SSM as an efficient fusion module, adept at capturing long-range cross-modal dependencies with linear complexity.

\noindent Our main contributions are threefold: (1) An automatic prompt construction pipeline that combines style clustering with an MLLM to produce fine-grained, decoupled identity and clothing descriptions;
(2) The BDAM, which achieves precise decoupling and alignment reinforced by a multi-task loss strategy combining an alignment loss and an orthogonality constraint based on HSIC;
and (3) The novel integration of a Mamba SSM fusion module that models long-range cross-modal dependencies with linear complexity.