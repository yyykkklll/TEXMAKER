\section{Experiments}
\label{sec:experiments}

\subsection{Datasets and Metrics}
\noindent \textbf{CUHK-PEDES}~\cite{liPersonSearchNaturalLanguageDescription2017} is the first benchmark for text-to-image person re-identification, comprising 40,206 images of 13,003 identities with two textual descriptions per image. Following the official protocol, we partition the dataset into training (11,003 identities), validation (1,000 identities), and testing (1,000 identities) sets.

\noindent \textbf{ICFG-PEDES}~\cite{zhuDSSLDeepSurroundings2021} consists of 54,552 images across 4,102 identities, each annotated with a single human-written description. The official split allocates 3,102 identities for training and 1,000 for testing.

\noindent \textbf{RSTPReid}~\cite{dingSemanticallySelfAlignedNetwork2021} captures 20,505 images of 4,104 identities from 15 cameras, with five viewpoint-diverse images and two descriptions per identity. We adopt the standard split: 3,701 identities for training and 200 each for validation and testing.

\noindent \textbf{Evaluation Metrics.} We report Rank-k accuracy (R-1, R-5, R-10) and mean Average Precision (mAP), consistent with prior works in this domain.

\subsection{Implementation Details}
\label{sec:4.2}
\noindent Our model employs pre-trained bert-base-uncased as the text encoder with hidden dimensionality of 768, and vit-base-patch16-224 as the visual encoder. All images are resized to $224 \times 224$ pixels. The BDAM module processes visual tokens through multi-layer self-attention and cross-attention, disentangling them into identity and clothing features of 768 dimensions each. A two-layer Mamba fusion module with input/output dimension 256, state dimension 16, and convolutional kernel size 4 integrates the multimodal representations. Dropout rate is set to 0.1 throughout. Training utilizes the Adam optimizer with learning rate $1 \times 10^{-4}$, weight decay $1 \times 10^{-3}$, and cosine annealing scheduling. The multi-task loss combines InfoNCE, triplet, clothing adversarial, HSIC decoupling, and gate regularization terms. We employ GradNorm for dynamic task weighting with $\alpha = 1.5$ and weight learning rate 0.025; task weights are normalized to sum to the number of tasks and clipped to $[10^{-4}, 10]$. Gradient norms are computed on the final shared layer, with an additional log-variance regularization coefficient of $1 \times 10^{-3}$.

For data augmentation, we extract style embeddings using clip-vit-base-patch32, cluster them via DBSCAN, and generate style-consistent identity and clothing descriptions through ChatGPT-4o. All splits are enforced at the identity level to prevent data leakage. Each experiment is repeated with random seeds 0, 1, and 2; we select the best checkpoint based on validation performance and report the mean across three runs.

\subsection{Parameter Analysis}
\label{sec:param_analysis}

\noindent Although the identity and clothing branches in BDAM adopt identical architectures, they learn distinctly different representations through independent parameterization and asymmetric supervision. The BDAM gating mechanism generates dimension-wise weights $g_{\text{dis}} \in \mathbb{R}^{B \times D}$ via $g_{\text{dis}} = \sigma(W_g[\hat{f}_{id}; \hat{f}_{clo}] + b_g)$, yielding complementary coefficients $W_{id} = g_{\text{dis}}$ and $W_{clo} = 1-g_{\text{dis}}$ that satisfy the zero-sum constraint $W_{id} + W_{clo} = \mathbf{1}$. This constraint introduces competitive pressure between branches, encouraging each to specialize in unique semantic patterns. The learning dynamics are governed by asymmetric loss signals: the identity branch receives supervision from InfoNCE and triplet losses, steering it toward person-invariant attributes such as body shape and posture, while the clothing branch is guided by matching and adversarial losses toward appearance-centric features. The HSIC-based decoupling loss enforces statistical independence by orthogonalizing the two feature subspaces in a Reproducing Kernel Hilbert Space, thereby preventing representational collapse.

Similarly, the fusion module employs an adaptive gating mechanism to balance modality contributions. The fusion gate $g_{\text{fus}} \in \mathbb{R}^{B \times 2}$ is computed as $g_{\text{fus}} = \text{Softmax}(W_f[f_{\text{img}}^{\text{gate}}; f_{\text{txt}}])$, producing normalized modality weights $W_{\text{img}} = g_{\text{fus}}[:, 0]$ and $W_{\text{txt}} = g_{\text{fus}}[:, 1]$ satisfying $W_{\text{img}} + W_{\text{txt}} = 1$. This sample-level gating mechanism operates at the instance level, dynamically adjusting modality importance based on input characteristics, whereas the disentanglement gate provides dimension-level control within the visual stream.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{gate_weight_distribution.pdf}
    \caption{Learned gate weight distributions on the CUHK-PEDES test set. (a) BDAM assigns significantly higher weights to identity features than clothing features, validating the identity-centric design. (b) Fusion module maintains balanced modality contributions, confirming effective cross-modal integration without modality dominance.}
    \label{fig:gate_distribution}
\end{figure}

To empirically validate these mechanisms, we analyze the distributions of learned gate weights across the test set, as shown in \cref{fig:gate_distribution}. The BDAM identity weights exhibit a right-skewed distribution with mean 0.62 and standard deviation 0.18, substantially exceeding the clothing weights which concentrate around 0.38. This asymmetry confirms that identity-relevant features dominate the final representation, aligning with our design objective. The bimodal separation pattern indicates the gate has learned to discriminate between samples with strong identity cues versus those requiring additional contextual information. In contrast, the fusion module maintains near-equal modality contributions with means of 0.53 for image and 0.47 for text, both with moderate variance around 0.12. This balanced distribution, centered near the uniform baseline of 0.5, demonstrates stable cross-modal alignment without collapse toward either modality. These empirical observations corroborate our theoretical design: BDAM enforces semantic disentanglement through asymmetric supervision and HSIC regularization, while the fusion gate achieves dynamic equilibrium via Softmax normalization and balanced loss weighting.

\subsection{Ablation Study}
\noindent We conduct systematic ablation studies to validate the contribution of each architectural component and loss term. All experiments are performed on CUHK-PEDES unless otherwise specified.

\begin{table}[htbp]
    \centering
    \caption{Ablation study on the BDAM module.}
    \label{tab:ablation_bdam}
    \vspace{-0.3cm}
    \setlength{\tabcolsep}{4pt}
    \footnotesize
    \begin{tabular}{lcccc}
        \toprule
        Method                 & mAP(\%) $\uparrow$ & R-1(\%) $\uparrow$ & R-5(\%) $\uparrow$ & R-10(\%) $\uparrow$ \\
        \midrule
        Baseline(w/o BDAM)     & 59.81              & 70.54              & 85.49              & 91.26               \\
        + BDAM                 & 66.74              & 76.27              & 89.30              & 94.02               \\
        \quad w/o Cross-Attn   & 62.56              & 71.39              & 87.05              & 92.98               \\
        \quad w/o Gate         & 65.11              & 74.63              & 88.77              & 93.56               \\
        \quad Shallow(3-layer) & 64.27              & 73.74              & 88.09              & 93.32               \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table}

\noindent \textbf{Disentanglement Module.} As shown in \cref{tab:ablation_bdam}, incorporating BDAM yields substantial improvements over the baseline across all metrics, demonstrating the effectiveness of explicit identity-clothing disentanglement. Within the BDAM architecture, both bidirectional cross-attention and the gating mechanism prove essential: removing cross-attention results in significant performance degradation, confirming its role in enabling semantic interaction between branches; ablating the gate mechanism similarly impairs retrieval accuracy. Furthermore, reducing the network depth to three layers degrades performance, indicating that deeper semantic modeling is necessary to achieve robust disentanglement. The learned gate weight distributions in \cref{fig:gate_distribution} provide additional evidence, showing that the gating mechanism automatically achieves semantic specialization in BDAM and modality balance in fusion through end-to-end training.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{disentanglement_tsne_academic.pdf}
    \caption{t-SNE visualization comparing feature distributions.
        (a) The baseline model's 'Entangled Features' [cite: 407] show poor separation, with clusters for different identities (e.g., ID 1090 [cite: 411] and ID 1099 [cite: 412]) positioned very closely.
        (b) With BDAM and HSIC regularization, the 'Disentangled Features'  show a clear separation: 'Clothing' features (marked by 'x')  occupy an independent region on the **left** , while identity features (dots)  form a distinct cluster on the **right**.}
    \label{fig:demo}
\end{figure}

To qualitatively assess disentanglement quality, we visualize learned features using t-SNE dimensionality reduction in \cref{fig:demo}. (a) The baseline model produces a highly mixed embedding space with extensive identity overlap[cite: 407], indicating severe entanglement between identity and appearance attributes. (b) In contrast, incorporating BDAM with HSIC regularization yields a well-structured feature space wherein identity embeddings form tight, well-separated clusters on the right, while clothing features occupy an independent region on the left[cite: 419, 430, 431, 432, 433, 434, 435, 436]. Within the identity cluster, same-person samples exhibit strong intra-class cohesion with clear inter-class boundaries[cite: 437, 438, 439, 440, 441]. This geometric structure confirms the orthogonality enforced by our disentanglement mechanism and provides visual evidence for the quantitative performance gains observed in \cref{tab:ablation_bdam}.

\begin{table}[htbp]
    \centering
    \caption{Ablation study on the fusion module.}
    \label{tab:ablation_fusion}
    \vspace{-0.3cm}
    \setlength{\tabcolsep}{3.5pt}
    \footnotesize
    \begin{tabular}{lcccc}
        \toprule
        Method               & mAP(\%) $\uparrow$ & R-1(\%) $\uparrow$ & R-5(\%) $\uparrow$ & R-10(\%) $\uparrow$ \\
        \midrule
        Baseline(w/o Fusion) & 59.81              & 70.54              & 85.49              & 91.26               \\
        Full Fusion          & 69.58              & 78.42              & 90.74              & 95.11               \\
        \quad w/o Mamba      & 66.89              & 75.73              & 89.06              & 93.92               \\
        \quad w/o Gate       & 68.64              & 77.58              & 90.11              & 94.87               \\
        \quad w/o Alignment  & 68.15              & 77.09              & 89.84              & 94.53               \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table}

\noindent \textbf{Fusion Module.} \cref{tab:ablation_fusion} demonstrates that the complete Mamba-based fusion module substantially outperforms the baseline. Among the three components, removing the Mamba SSM backbone incurs the largest performance drop, highlighting its critical role in modeling long-range dependencies across modalities. Ablating either the gating mechanism or modality alignment layers results in more modest degradation, suggesting these components provide complementary benefits by enabling adaptive weighting and distributional alignment, respectively.

\begin{table}[htbp]
    \centering
    \caption{Ablation study on individual loss components.}
    \label{tab:ablation_loss}
    \vspace{-0.3cm}
    \setlength{\tabcolsep}{4pt}
    \footnotesize
    \begin{tabular}{lcccc}
        \toprule
        Method          & mAP(\%) $\uparrow$ & R-1(\%) $\uparrow$ & R-5(\%) $\uparrow$ & R-10(\%) $\uparrow$ \\
        \midrule
        \rowcolor{gray!20}
        Full Model      & 72.61              & 79.93              & 92.95              & 96.47               \\
        w/o InfoNCE     & 28.14              & 36.55              & 55.21              & 65.83               \\
        w/o Triplet     & 67.22              & 74.89              & 88.15              & 93.12               \\
        w/o Adversarial & 69.15              & 76.92              & 89.53              & 94.22               \\
        w/o Decoupling  & 70.03              & 77.81              & 90.11              & 94.98               \\
        w/o Gate Reg.   & 71.98              & 79.23              & 91.35              & 95.71               \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table}

\noindent \textbf{Loss Functions.} To validate the necessity of each loss component, we systematically remove individual terms while keeping all other settings fixed. \cref{tab:ablation_loss} reveals that every loss term contributes positively to final performance. The InfoNCE and triplet losses emerge as foundational: removing InfoNCE causes catastrophic performance collapse, underscoring the critical importance of cross-modal contrastive alignment; ablating the triplet loss also significantly degrades retrieval accuracy, confirming the necessity of strong intra-modal identity discrimination. The disentanglement-specific losses—adversarial and HSIC decoupling—are likewise essential; removing either term impairs performance, demonstrating that architectural design alone is insufficient and explicit loss-based constraints are required to enforce identity-clothing separation. Finally, removing gate regularization causes minor but consistent degradation, validating its auxiliary role in stabilizing the learned gating mechanisms.

\begin{table}[htbp]
    \centering
    \caption{Ablation study on clustering strategies for style prompt generation.}
    \label{tab:ablation_style_prompts}
    \vspace{-0.3cm}
    \setlength{\tabcolsep}{4pt}
    \footnotesize
    \begin{tabular}{lcccc}
        \toprule
        Method               & mAP(\%) $\uparrow$ & R-1(\%) $\uparrow$ & R-5(\%) $\uparrow$ & R-10(\%) $\uparrow$ \\
        \midrule
        Baseline (w/o Style) & 71.22              & 79.18              & 91.51              & 95.79               \\
        + Random             & 71.95              & 79.54              & 91.66              & 95.82               \\
        + K-Means            & 71.85              & 79.20              & 91.60              & 95.90               \\
        + GMM                & 71.70              & 79.05              & 91.50              & 95.75               \\
        + Agglomerative      & 71.78              & 79.10              & 91.55              & 95.80               \\
        + HDBSCAN            & 72.20              & 79.40              & 91.95              & 96.10               \\
        \rowcolor{gray!20}
        + DBSCAN (Ours)      & 72.61              & 79.93              & 92.95              & 96.47               \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table}

\noindent \textbf{Style Prompt Generation.} We compare various clustering algorithms for generating style-consistent prompts, as detailed in \cref{tab:ablation_style_prompts}. While random sampling provides minimal gains over the baseline, demonstrating the importance of structured prompt design, centroid-based methods such as K-Means and Gaussian Mixture Models offer modest improvements but require manual specification of cluster counts and struggle with irregular density distributions. Hierarchical agglomerative clustering exhibits similar limitations. In contrast, density-based approaches—particularly DBSCAN—achieve superior performance by automatically discovering arbitrarily-shaped clusters while identifying and filtering noise. DBSCAN's ability to adaptively determine the number of style categories without hyperparameter tuning makes it the optimal choice for this task.

\subsection{Efficiency Analysis}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{Figure6.pdf}
    \caption{Efficiency-performance comparison on CUHK-PEDES. All metrics normalized to [0, 100] where higher is better (cost metrics inverted: lower resource usage → higher score). Our Mamba-based fusion achieves competitive accuracy with minimal computational overhead.}
    \label{fig:efficiency_comparison}
\end{figure}

\noindent As shown in \cref{fig:efficiency_comparison}, our Mamba-based fusion module achieves an optimal balance in the accuracy-efficiency trade-off space. All metrics are normalized such that higher scores consistently indicate superior performance—for cost-related metrics (Params, FLOPs, Memory), lower resource consumption translates to higher normalized scores, while performance metrics (mAP, Speed) are directly normalized.

\noindent The visualization reveals that Mamba maintains near-peak performance across all dimensions. Compared to Transformer-4Layer, our method delivers competitive retrieval accuracy (reflected in comparable mAP scores) while achieving substantially higher efficiency scores in Params, FLOPs, and Memory—confirming lower computational overhead. Against the Simple baseline, Mamba demonstrates marked mAP improvements while maintaining comparable efficiency scores, validating superior resource utilization.

\subsection{Comparisons with State-of-the-Art Methods}
\begin{table*}[!t]
    \centering
    \caption{Performance comparison with state-of-the-art methods on three benchmark datasets.}
    \label{tab:sota_comparison}
    \vspace{-0.3cm}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l|c|cccc|cccc|cccc}
            \thickhline
            \multirow{2}{*}{\textbf{Method}}
                                                                        & \multirow{2}{*}{\textbf{Backbone}}
                                                                        & \multicolumn{4}{c|}{\textbf{CUHK-PEDES}}
                                                                        & \multicolumn{4}{c|}{\textbf{ICFG-PEDES}}
                                                                        & \multicolumn{4}{c}{\textbf{RSTPReid}}                                                                                                                                                                                 \\
            \hhline{~~|----|----|----}                                  &
                                                                        & \textbf{R-1}                             & \textbf{R-5}   & \textbf{R-10}  & \textbf{mAP}   & \textbf{R-1} & \textbf{R-5} & \textbf{R-10} & \textbf{mAP} & \textbf{R-1} & \textbf{R-5} & \textbf{R-10} & \textbf{mAP} \\
            \hline
            \multicolumn{14}{l}{\small\textit{Methods with CLIP backbone:}}                                                                                                                                                                                                                     \\
            \hline
            IRRA~\cite{jiangCrossModalImplicitRelation2023}             & CLIP-ViT
                                                                        & 73.38                                    & 89.93          & 93.71          & 66.10          & 63.36        & 80.82        & 85.82         & 38.06        & 60.20        & 81.30        & 88.20         & 47.17        \\
            IRLT~\cite{liuCausalityInspiredInvariantRepresentation2024} & CLIP-ViT
                                                                        & 73.67                                    & 89.71          & 93.57          & 65.94          & 63.57        & 80.57        & 86.32         & 38.34        & 60.51        & 82.85        & 89.71         & 47.64        \\
            CFAM~\cite{zuoUFineBenchTowardsTextbased2024}               & CLIP-ViT
                                                                        & 74.46                                    & 90.19          & 94.01          & -              & 64.72        & 81.35        & 86.31         & -            & 61.49        & 82.26        & 89.23         & -            \\
            Propot~\cite{yanPrototypicalPromptingTexttoimage2024}       & CLIP-ViT
                                                                        & 74.89                                    & 89.90          & 94.17          & 67.12          & 65.12        & 81.57        & 86.97         & 42.93        & 61.87        & 83.63        & 89.70         & 47.82        \\
            RDE~\cite{qinNoisyCorrespondenceLearningTexttoImage2024}    & CLIP-ViT
                                                                        & 75.94                                    & 90.14          & 94.12          & 67.56          & 67.68        & 82.47        & 87.36         & 40.06        & 65.35        & 83.95        & 89.90         & 50.88        \\
            HAM~\cite{jiangModelingThousandsHuman2025}                  & CLIP-ViT
                                                                        & 77.99                                    & 91.34          & 95.03          & 69.72          & 69.95        & 83.88        & 88.39         & 42.72        & 72.50        & 87.70        & 91.95         & 55.47        \\
            \hline
            \multicolumn{14}{l}{\small\textit{Methods with ViT backbone:}}                                                                                                                                                                                                                      \\
            \hline
            CPCL~\cite{zhengCPCLCrossModalPrototypical2024}             & ViT
                                                                        & 70.03                                    & 87.28          & 91.78          & 63.19          & 62.60        & 79.07        & 84.46         & 36.16        & 58.35        & 81.05        & 87.65         & 45.81        \\
            PDReid~\cite{liPromptDecouplingTexttoImage2024}             & ViT
                                                                        & 71.59                                    & 87.95          & 92.45          & 65.03          & 60.93        & 77.96        & 84.11         & 36.44        & 56.65        & 77.40        & 84.70         & 45.27        \\
            SSAN~\cite{dingSemanticallySelfAlignedNetwork2021}          & ViT
                                                                        & 61.37                                    & 80.15          & 86.73          & -              & 54.23        & 72.63        & 79.53         & -            & 43.50        & 67.80        & 77.15         & -            \\
            CFine~\cite{yanCLIPDrivenFinegrainedTextImage2022}          & ViT
                                                                        & 69.57                                    & 85.93          & 91.15          & -              & 60.83        & 76.55        & 82.42         & -            & 50.55        & 72.50        & 81.60         & -            \\
            IVT~\cite{shuSeeFinerSeeMore2022}                           & ViT
                                                                        & 65.59                                    & 83.11          & 89.21          & -              & 56.04        & 73.60        & 80.22         & -            & 46.70        & 70.00        & 78.80         & -            \\
            \thickhline
            \rowcolor{gray!20}
            \textbf{Ours}                                               & ViT
                                                                        & \textbf{79.93}                           & \textbf{92.95} & \textbf{96.47} & \textbf{72.61}
                                                                        & \textbf{68.68}                           & \textbf{84.29} & \textbf{89.74} & \textbf{41.78}
                                                                        & \textbf{74.33}                           & \textbf{88.85} & \textbf{92.95} & \textbf{57.68}                                                                                                                           \\
            \thickhline
        \end{tabular}
    }
    \vspace{-0.2cm}
\end{table*}

\noindent We compare our method against recent state-of-the-art approaches across three widely-adopted benchmarks, as summarized in \cref{tab:sota_comparison}. On CUHK-PEDES, our method establishes new state-of-the-art results, substantially surpassing both traditional ViT-based methods and the recent CLIP-based HAM baseline. Notably, this performance is achieved without leveraging large-scale pre-trained vision-language models, demonstrating that our identity-clothing disentanglement architecture and efficient fusion strategy provide effective cross-modal alignment through task-specific design. On ICFG-PEDES, our approach remains competitive with HAM while significantly outperforming other recent methods, further validating the robustness of decoupled modeling under clothing variation. On RSTPReid, we achieve the best reported results to date, surpassing all ViT-based methods and establishing clear improvements over HAM. These consistent gains across diverse datasets—varying in scale, camera setup, and annotation density—underscore the generalization capability of our disentanglement and fusion mechanisms in complex, multi-camera, cross-scene scenarios. In summary, our method delivers superior or highly competitive performance across all three benchmarks, validating the effectiveness of the Bidirectional Decoupling Alignment Module, Mamba-based fusion, and dynamic multi-task optimization for text-to-image person re-identification.