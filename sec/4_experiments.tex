\section{Experiments}
\label{sec:experiments}

\subsection{Datasets and Metrics}
\noindent \textbf{CUHK-PEDES}~\cite{liPersonSearchNaturalLanguageDescription2017} is the first benchmark for text-to-image person re-identification, comprising 40,206 images of 13,003 identities with two textual descriptions per image. Following the official protocol, we partition the dataset into training (11,003 identities), validation (1,000 identities), and testing (1,000 identities) sets.

\noindent \textbf{ICFG-PEDES}~\cite{zhuDSSLDeepSurroundings2021} consists of 54,552 images across 4,102 identities, each annotated with a single human-written description. The official split allocates 3,102 identities for training and 1,000 for testing.

\noindent \textbf{RSTPReid}~\cite{dingSemanticallySelfAlignedNetwork2021} captures 20,505 images of 4,104 identities from 15 cameras, with five viewpoint-diverse images and two descriptions per identity. We adopt the standard split: 3,701 identities for training and 200 each for validation and testing.

\noindent \textbf{Evaluation Metrics.} We report Rank-k accuracy (R-1, R-5, R-10) and mean Average Precision (mAP), consistent with prior works in this domain.

\subsection{Implementation Details}
\label{sec:4.2}
\noindent Our model employs the \texttt{bert-base-uncased} model, which is pre-trained, as the text encoder with a hidden dimensionality of 768. We use \texttt{vit-base-patch16-224} as the visual encoder. All images are resized to $224 \times 224$ pixels. The BDAM module processes visual tokens through multiple layers of attention, disentangling them into identity and clothing features of 768 dimensions each. A Mamba fusion module with two layers, configured with an input and output dimension of 256, a state dimension of 16, and a convolutional kernel size of 4, integrates the multimodal representations. The dropout rate is set to 0.1 throughout.

\noindent Training utilizes the Adam optimizer with a learning rate of $1 \times 10^{-4}$, weight decay of $1 \times 10^{-3}$, and cosine annealing scheduling. The loss function for multiple tasks combines InfoNCE, triplet, clothing alignment, HSIC decoupling, and gate regularization terms. We employ GradNorm for dynamic task weighting with $\alpha = 1.5$ and a weight learning rate of 0.025. Task weights are normalized to sum to the number of tasks and clipped to the range $[10^{-4}, 10]$. Gradient norms are computed on the final shared layer, with an additional regularization coefficient for log variance of $1 \times 10^{-3}$.

\noindent For data augmentation, we extract style embeddings using \texttt{clip-vit-base-patch32}, cluster them via DBSCAN, and generate descriptions consistent with the style for identity and clothing through ChatGPT-4. All splits are enforced at the identity level to prevent data leakage. Each experiment is repeated with random seeds 0, 1, and 2; we select the best checkpoint based on validation performance and report the mean across the three runs.

\subsection{Parameter Analysis}
\label{sec:param_analysis}
\noindent Although the identity and clothing stream in BDAM adopt identical architectures, they learn distinctly different representations through independent parameterization and asymmetric supervision.
The BDAM gating mechanism generates weights at the dimension level $g_{\text{dis}} \in \mathbb{R}^{B \times D}$ via $g_{\text{dis}} = \sigma(W_g[\hat{f}_{id}; \hat{f}_{clo}] + b_g)$. This yields complementary coefficients $W_{id} = g_{\text{dis}}$ and $W_{clo} = 1-g_{\text{dis}}$ that satisfy the constraint $W_{id} + W_{clo} = \mathbf{1}$.
This constraint introduces competitive pressure between stream, encouraging each to specialize in unique semantic patterns.

\noindent The learning dynamics are governed by asymmetric loss signals. The identity stream receives supervision from InfoNCE and triplet losses, steering it toward attributes that are invariant to the person, such as body shape and posture. In contrast, the clothing stream is guided by matching and alignment losses toward features centered on appearance. The decoupling loss based on HSIC enforces statistical independence by orthogonalizing the two feature subspaces in a Reproducing Kernel Hilbert Space, thereby preventing representational collapse.

\noindent Similarly, the fusion module employs an adaptive gating mechanism to balance modality contributions.
The fusion gate $g_{\text{fus}} \in \mathbb{R}^{B \times 2}$ is computed as $g_{\text{fus}} = \text{Softmax}(W_f[f_{\text{img}}; f_{\text{txt}}])$, producing normalized modality weights $W_{\text{img}} = g_{\text{fus}}[:, 0]$ and $W_{\text{txt}} = g_{\text{fus}}[:, 1]$ satisfying $W_{\text{img}} + W_{\text{txt}} = 1$.
This mechanism operates at the instance level, dynamically adjusting modality importance based on input characteristics, whereas the disentanglement gate provides control at the dimension level within the visual stream.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{gate_weight_distribution.pdf}
    \caption{Learned gate weight distributions on the CUHK-PEDES test set.
        (a) BDAM assigns significantly higher weights to identity features than clothing features, validating the identity-centric design.
        (b) Fusion module maintains balanced modality contributions, confirming effective cross-modal integration without modality dominance.}
    \label{fig:gate_distribution}
\end{figure}
\noindent To empirically validate these mechanisms, we analyze the distributions of learned gate weights across the test set, as shown in \cref{fig:gate_distribution}.
The BDAM identity weights exhibit a distribution skewed to the right, with a mean of 0.61 and standard deviation of 0.17, substantially exceeding the clothing weights which concentrate around 0.38.
This asymmetry confirms that features relevant to identity dominate the final representation, aligning with our design centered on identity.
The bimodal separation pattern indicates the gate has learned to discriminate between samples with strong identity cues versus those requiring additional contextual information.
In contrast, the fusion module maintains nearly equal modality contributions, with means of 0.52 and 0.48, both with moderate variance around 0.10.
This balanced distribution, centered near the uniform baseline of 0.5, demonstrates stable alignment between modalities without collapse toward either one.

\noindent These empirical observations corroborate our theoretical design: BDAM enforces semantic disentanglement through asymmetric supervision and regularization via HSIC, while the fusion gate achieves dynamic equilibrium via Softmax normalization and balanced loss weighting.

\subsection{Ablation Study}
\noindent We conduct systematic ablation studies to validate the contribution of each architectural component and loss term. All experiments are performed on CUHK-PEDES unless otherwise specified.

\begin{table}[htbp]
    \centering
    \caption{Ablation study on the BDAM module.}
    \label{tab:ablation_bdam}
    \vspace{-0.3cm}
    \setlength{\tabcolsep}{4pt}
    \footnotesize
    \begin{tabular}{lcccc}
        \toprule
        Method                 & mAP(\%) $\uparrow$ & R-1(\%) $\uparrow$ & R-5(\%) $\uparrow$ & R-10(\%) $\uparrow$ \\
        \midrule
        Baseline(w/o BDAM)     & 59.81              & 70.54              & 85.49              & 91.26               \\
        + BDAM                 & 66.74              & 76.27              & 89.30              & 94.02               \\
        \quad w/o Cross-Attn   & 62.56              & 71.39              & 87.05              & 92.98               \\
        \quad w/o Gate         & 65.11              & 74.63              & 88.77              & 93.56               \\
        \quad Shallow(3-layer) & 64.27              & 73.74              & 88.09              & 93.32               \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table}

\noindent \textbf{Disentanglement Module.} As shown in \cref{tab:ablation_bdam}, incorporating BDAM yields substantial improvements over the baseline, demonstrating the effectiveness of explicit disentanglement between identity and clothing.
The subsequent analysis of its components confirms the necessity of each design choice. 
Removing the bidirectional cross attention, ablating the gating mechanism, or reducing the network depth to three layers all result in significant performance degradation. 
This indicates that the semantic interaction between stream, the adaptive feature control, and sufficient model depth are all essential for achieving robust disentanglement.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{disentanglement_tsne_academic.pdf}
    \caption{Visualization of feature distributions using t-SNE. (a) The baseline model exhibits highly entangled features with poor identity separation. (b) Our method (BDAM with HSIC) yields a highly organized space, clearly decoupling identity (dots) from clothing (crosses) features.}
    \label{fig:demo}
\end{figure}

\noindent To provide a qualitative assessment of disentanglement quality, we visualize the learned feature space using dimensionality reduction via t-SNE in \cref{fig:demo}.
The visualization reveals a stark contrast. The baseline model (a) produces a highly mixed embedding space where identities overlap, indicating severe entanglement between identity and appearance attributes.
(b) Conversely, our method, incorporating BDAM with HSIC regularization, yields a highly organized feature space. 
The identity embeddings form tight clusters, exhibiting strong cohesion for samples of the same person and clear separation between different identities. 
Concurrently, clothing features are projected to an independent region.
This geometric structure confirms the orthogonality enforced by our disentanglement mechanism and provides visual evidence for the quantitative performance gains observed in \cref{tab:ablation_bdam}.

\begin{table}[htbp]
    \centering
    \caption{Ablation study on the fusion module.}
    \label{tab:ablation_fusion}
    \vspace{-0.3cm}
    \setlength{\tabcolsep}{3.5pt}
    \footnotesize
    \begin{tabular}{lcccc}
        \toprule
        Method               & mAP(\%) $\uparrow$ & R-1(\%) $\uparrow$ & R-5(\%) $\uparrow$ & R-10(\%) $\uparrow$ \\
        \midrule
        Baseline(w/o Fusion) & 59.81              & 70.54              & 85.49              & 91.26               \\
        Full Fusion          & 69.58              & 78.42              & 90.74              & 95.11               \\
        \quad w/o Mamba      & 66.89              & 75.73              & 89.06              & 93.92               \\
        \quad w/o Gate       & 68.64              & 77.58              & 90.11              & 94.87               \\
        \quad w/o Alignment  & 68.15              & 77.09              & 89.84              & 94.53               \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table}

\noindent \textbf{Fusion Module.} The analysis in \cref{tab:ablation_fusion} confirms that the effectiveness of our complete fusion module, which substantially outperforms the baseline, stems from the synergy of its three components.
The Mamba SSM backbone is identified as the most critical element; 
its removal incurs the largest performance drop, highlighting the importance of modeling dependencies over long ranges across modalities.
Furthermore, the gating mechanism and modality alignment layers provide essential complementary benefits. 
Ablating either component results in a clear degradation, confirming their respective roles in enabling adaptive weighting and achieving distributional alignment.

\begin{table}[htbp]
    \centering
    \caption{Ablation study on individual loss components.}
    \label{tab:ablation_loss}
    \vspace{-0.3cm}
    \setlength{\tabcolsep}{4pt}
    \footnotesize
    \begin{tabular}{lcccc}
        \toprule
        Method         & mAP(\%) $\uparrow$ & R-1(\%) $\uparrow$ & R-5(\%) $\uparrow$ & R-10(\%) $\uparrow$ \\
        \midrule
        \rowcolor{gray!20}
        Full Model     & 72.61              & 79.93              & 92.95              & 96.47               \\
        w/o InfoNCE    & 28.14              & 36.55              & 55.21              & 65.83               \\
        w/o Triplet    & 67.22              & 74.89              & 88.15              & 93.12               \\
        w/o Alignment  & 69.15              & 76.92              & 89.53              & 94.22               \\
        w/o Decoupling & 70.03              & 77.81              & 90.11              & 94.98               \\
        w/o Gate Reg.  & 71.98              & 79.23              & 91.35              & 95.71               \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table}

\noindent \textbf{Loss Functions.} To validate the necessity of each loss component, we systematically remove individual terms while keeping all other settings fixed. 
As shown in \cref{tab:ablation_loss}, every loss term contributes positively to the final performance.
The InfoNCE and triplet losses emerge as foundational. 
Removing InfoNCE causes a catastrophic performance collapse, underscoring the critical importance of contrastive alignment between modalities. 
Ablating the triplet loss also significantly degrades retrieval accuracy, confirming the necessity of strong identity discrimination within a modality.
The losses specific to disentanglement, namely the alignment and HSIC decoupling losses, are likewise essential. 
The performance degradation upon removing either term demonstrates that architectural design alone is insufficient; explicit constraints imposed by the loss function are required to enforce the separation of identity and clothing.
Finally, the minor but consistent degradation from removing gate regularization validates its auxiliary role in stabilizing the learned gating mechanisms.

\begin{table}[htbp]
    \centering
    \caption{Ablation study on clustering strategies for style prompt generation.}
    \label{tab:ablation_style_prompts}
    \vspace{-0.3cm}
    \setlength{\tabcolsep}{4pt}
    \footnotesize
    \begin{tabular}{lcccc}
        \toprule
        Method               & mAP(\%) $\uparrow$ & R-1(\%) $\uparrow$ & R-5(\%) $\uparrow$ & R-10(\%) $\uparrow$ \\
        \midrule
        Baseline (w/o Style) & 71.22              & 79.18              & 91.51              & 95.79               \\
        + Random             & 71.95              & 79.54              & 91.66              & 95.82               \\
        + K-Means            & 71.85              & 79.20              & 91.60              & 95.90               \\
        + GMM                & 71.70              & 79.05              & 91.50              & 95.75               \\
        + Agglomerative      & 71.78              & 79.10              & 91.55              & 95.80               \\
        + HDBSCAN            & 72.20              & 79.40              & 91.95              & 96.10               \\
        \rowcolor{gray!20}
        + DBSCAN (Ours)      & 72.61              & 79.93              & 92.95              & 96.47               \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table}

\noindent \textbf{Style Prompt Generation.} We compare various clustering algorithms for generating prompts consistent with style, as detailed in \cref{tab:ablation_style_prompts}.
While random sampling provides minimal gains over the baseline, demonstrating the importance of a structured prompt design, traditional methods such as K-Means, Gaussian Mixture Models, and Hierarchical Agglomerative clustering offer only modest improvements.
These methods are constrained by limitations such as the manual specification of cluster counts and difficulty handling irregular density distributions.
In contrast, approaches based on density, particularly DBSCAN, achieve superior performance. 
This is because DBSCAN automatically discovers clusters of arbitrary shape while identifying and filtering noise.
DBSCAN's ability to adaptively determine the number of style categories without requiring hyperparameter tuning makes it the optimal choice for this task.

\subsection{Efficiency Analysis}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{Figure6.pdf}
    \caption{Comparison of efficiency and performance on CUHK-PEDES. All metrics are normalized to [0, 100], where higher is better. Cost metrics (Params, FLOPs, Memory) are inverted, meaning lower resource usage yields a higher score. Our fusion module employing Mamba achieves competitive accuracy with reduced computational overhead.}
    \label{fig:efficiency_comparison}
\end{figure}

\noindent As shown in \cref{fig:efficiency_comparison}, our fusion module employing Mamba achieves an optimal balance in the trade-off between accuracy and efficiency.
The visualization reveals that Mamba maintains performance near the peak across all dimensions.
Compared to the four-layer Transformer baseline, our method delivers competitive retrieval accuracy (reflected in comparable mAP scores) while achieving substantially higher efficiency scores in Params, FLOPs, and Memory, which confirms lower computational overhead.
Against the Simple baseline, Mamba demonstrates marked mAP improvements while maintaining comparable efficiency scores, validating superior resource utilization.

\subsection{Comparisons with State-of-the-Art Methods}
\begin{table*}[!t]
    \centering
    \caption{Performance comparison with state-of-the-art methods on three benchmark datasets.}
    \label{tab:sota_comparison}
    \vspace{-0.3cm}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l|c|cccc|cccc|cccc}
            \thickhline
            \multirow{2}{*}{\textbf{Method}}
                                                                        & \multirow{2}{*}{\textbf{Backbone}}
                                                                        & \multicolumn{4}{c|}{\textbf{CUHK-PEDES}}
                                                                        & \multicolumn{4}{c|}{\textbf{ICFG-PEDES}}
                                                                        & \multicolumn{4}{c}{\textbf{RSTPReid}}                                                                                                                                                                                 \\
            \hhline{~~|----|----|----}                                  &
                                                                        & \textbf{R-1}                             & \textbf{R-5}   & \textbf{R-10}  & \textbf{mAP}   & \textbf{R-1} & \textbf{R-5} & \textbf{R-10} & \textbf{mAP} & \textbf{R-1} & \textbf{R-5} & \textbf{R-10} & \textbf{mAP} \\
            \hline
            \multicolumn{14}{l}{\small\textit{Methods with CLIP backbone:}}                                                                                                                                                                                                                     \\
            \hline
            IRRA~\cite{jiangCrossModalImplicitRelation2023}             & CLIP-ViT
                                                                        & 73.38                                    & 89.93          & 93.71          & 66.10          & 63.36        & 80.82        & 85.82         & 38.06        & 60.20        & 81.30        & 88.20         & 47.17        \\
            IRLT~\cite{liuCausalityInspiredInvariantRepresentation2024} & CLIP-ViT
                                                                        & 73.67                                    & 89.71          & 93.57          & 65.94          & 63.57        & 80.57        & 86.32         & 38.34        & 60.51        & 82.85        & 89.71         & 47.64        \\
            CFAM~\cite{zuoUFineBenchTowardsTextbased2024}               & CLIP-ViT
                                                                        & 74.46                                    & 90.19          & 94.01          & -              & 64.72        & 81.35        & 86.31         & -            & 61.49        & 82.26        & 89.23         & -            \\
            Propot~\cite{yanPrototypicalPromptingTexttoimage2024}       & CLIP-ViT
                                                                        & 74.89                                    & 89.90          & 94.17          & 67.12          & 65.12        & 81.57        & 86.97         & 42.93        & 61.87        & 83.63        & 89.70         & 47.82        \\
            RDE~\cite{qinNoisyCorrespondenceLearningTexttoImage2024}    & CLIP-ViT
                                                                        & 75.94                                    & 90.14          & 94.12          & 67.56          & 67.68        & 82.47        & 87.36         & 40.06        & 65.35        & 83.95        & 89.90         & 50.88        \\
            HAM~\cite{jiangModelingThousandsHuman2025}                  & CLIP-ViT
                                                                        & 77.99                                    & 91.34          & 95.03          & 69.72          & 69.95        & 83.88        & 88.39         & 42.72        & 72.50        & 87.70        & 91.95         & 55.47        \\
            \hline
            \multicolumn{14}{l}{\small\textit{Methods with ViT backbone:}}                                                                                                                                                                                                                      \\
            \hline
            CPCL~\cite{zhengCPCLCrossModalPrototypical2024}             & ViT
                                                                        & 70.03                                    & 87.28          & 91.78          & 63.19          & 62.60        & 79.07        & 84.46         & 36.16        & 58.35        & 81.05        & 87.65         & 45.81        \\
            PDReid~\cite{liPromptDecouplingTexttoImage2024}             & ViT
                                                                        & 71.59                                    & 87.95          & 92.45          & 65.03          & 60.93        & 77.96        & 84.11         & 36.44        & 56.65        & 77.40        & 84.70         & 45.27        \\
            SSAN~\cite{dingSemanticallySelfAlignedNetwork2021}          & ViT
                                                                        & 61.37                                    & 80.15          & 86.73          & -              & 54.23        & 72.63        & 79.53         & -            & 43.50        & 67.80        & 77.15         & -            \\
            CFine~\cite{yanCLIPDrivenFinegrainedTextImage2022}          & ViT
                                                                        & 69.57                                    & 85.93          & 91.15          & -              & 60.83        & 76.55        & 82.42         & -            & 50.55        & 72.50        & 81.60         & -            \\
            IVT~\cite{shuSeeFinerSeeMore2022}                           & ViT
                                                                        & 65.59                                    & 83.11          & 89.21          & -              & 56.04        & 73.60        & 80.22         & -            & 46.70        & 70.00        & 78.80         & -            \\
            \thickhline
            \rowcolor{gray!20}
            \textbf{Ours}                                               & ViT
                                                                        & \textbf{79.93}                           & \textbf{92.95} & \textbf{96.47} & \textbf{72.61}
                                                                        & \textbf{68.68}                           & \textbf{84.29} & \textbf{89.74} & \textbf{41.78}
                                                                        & \textbf{74.33}                           & \textbf{88.85} & \textbf{92.95} & \textbf{57.68}                                                                                                                           \\
            \thickhline
        \end{tabular}
    }
    \vspace{-0.2cm}
\end{table*}

\noindent We compare our method against recent state-of-the-art approaches across three benchmarks, as summarized in \cref{tab:sota_comparison}. On CUHK-PEDES, our method establishes new state-of-the-art results, substantially surpassing both ViT-based methods and the recent CLIP-based HAM baseline. On ICFG-PEDES, our approach remains competitive with HAM while significantly outperforming other methods. On RSTPReid, we achieve the best reported results to date, surpassing all ViT-based methods and establishing clear improvements over HAM.

\noindent These consistent gains across diverse datasets underscore the generalization capability of our approach. Notably, this high performance is achieved without leveraging large-scale pre-trained vision-language models, demonstrating that our identity-clothing disentanglement architecture and efficient fusion strategy provide effective cross-modal alignment through task-specific design. In summary, our method delivers superior or highly competitive performance across all three benchmarks, validating the effectiveness of our proposed components for text-to-image person re-identification.