\section{Conclusion and Limitations}
\label{sec:conclusion}
\noindent In this work, we proposed a novel framework to address clothing-induced interference in text-to-image person re-identification, centered on MLLM-supervised feature decoupling. We successfully demonstrated that a Multimodal Large Language Model can provide fine-grained supervision to guide our Bidirectional Decoupling Alignment Module. By design, this module explicitly isolates identity-relevant features while actively suppressing clothing-related interference through a combined adversarial and kernel-based orthogonal loss strategy. Furthermore, our integration of a Mamba State Space Model proved to be an effective and efficient fusion strategy, adept at capturing cross-modal dependencies. Our method's effectiveness was validated by achieving new state-of-the-art performance on the CUHK-PEDES and RSTPReid benchmarks

Nonetheless, limitations remain, primarily concerning the potential for noise in MLLM-generated descriptions and the need for further optimization for large-scale deployment. Future work will focus on improving description reliability to mitigate noise and enhancing inference efficiency. We also plan to explore the framework's applicability to more challenging scenarios, such as clothing-changing person Re-ID, to further test the robustness of our decoupling mechanism.